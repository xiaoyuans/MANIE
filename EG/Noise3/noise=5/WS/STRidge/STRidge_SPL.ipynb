{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2990296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "from random import shuffle\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9b6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generate(M,P_PDG,W,N,K,Noise):\n",
    "    #W是原数据集,代表哪些代理之间有连接，即网络\n",
    "    #N是W的行数，即代理的个数\n",
    "    #代理人的博弈策略\n",
    "    player = zeros((M+1,N))\n",
    "    #原始博弈策略放在第一行，随机设置\n",
    "    for i in range(N):\n",
    "        if random.random()<=0.5:\n",
    "            player[0,i] = 1 #1代表合作\n",
    "        else:\n",
    "            player[0,i] = 0 #0代表不合作\n",
    "    #计算每个节点的收益\n",
    "    #M轮博弈\n",
    "    F = zeros((1,N))\n",
    "    G = zeros((M,N))\n",
    "    A = zeros((N,M,N))\n",
    "    for t in range(M):\n",
    "        for i in range(N):\n",
    "            if player[t,i] == 1:\n",
    "                s1 = array([[1],[0]])\n",
    "            else:\n",
    "                s1 = array([[0],[1]])\n",
    "            for j in range(N):\n",
    "                if player[t,j] == 0:\n",
    "                    s2 = array([[0],[1]])\n",
    "                else:\n",
    "                    s2 = array([[1],[0]])\n",
    "                F[0,j] = ((s1.T).dot(P_PDG)).dot(s2) #如果代理i和j连接，则代理i的收益为F[0,j]\n",
    "            A[i,t,:] = F  #A:N*M*N   \n",
    "            # F是三维矩阵A的第i页，第t行\n",
    "            G[t,i] = F.dot(W[:,i]) #第t轮代理i的收益\n",
    "        # update strategies\n",
    "        for k in range(N):\n",
    "            s=[i for i,x in enumerate(list(W[:,k])) if x>=1]     # 找出与代理k合作的代理的索引\n",
    "            if len(s)!=0: # 如果有代理与代理k合作\n",
    "                shuffle(s)\n",
    "                P = 1/(1+math.e**((G[t,k]-G[t,s[0]])/K)) # 费米规则\n",
    "                if random.random()<= P:\n",
    "                    player[t+1,k] = player[t,s[0]]\n",
    "                else:\n",
    "                    player[t+1,k] = player[t,k]\n",
    "            else: # 如果没有代理与代理k合作\n",
    "                player[t+1,k] = player[t,k]\n",
    "    # add noise for G\n",
    "    Aa = G + Noise*random.random((M,N)) # m轮收益总矩阵：包含每一轮的收益M*N\n",
    "    return [A,Aa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40594aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge(R, Ut, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "    \"\"\"\n",
    "    This function trains a predictor using STRidge.\n",
    "\n",
    "    It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "    using a loss function on a holdout set.\n",
    "\n",
    "    Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "    not squared 2-norm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "    np.random.seed(0) # for consistancy\n",
    "    n,_ = R.shape\n",
    "    train = np.random.choice(n, int(n*split), replace = False)\n",
    "    test = [i for i in np.arange(n) if i not in train]\n",
    "    TrainR = R[train,:]\n",
    "    TestR = R[test,:]\n",
    "    TrainY = Ut[train,:]\n",
    "    TestY = Ut[test,:]\n",
    "    D = TrainR.shape[1]       \n",
    "\n",
    "    # Set up the initial tolerance and l0 penalty\n",
    "    d_tol = float(d_tol)\n",
    "    tol = d_tol\n",
    "    if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
    "\n",
    "    # Get the standard least squares estimator\n",
    "    w = np.zeros((D,1))\n",
    "    w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
    "    err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "    tol_best = 0\n",
    "\n",
    "    # Now increase tolerance until test performance decreases\n",
    "    for iter in range(maxit):\n",
    "\n",
    "        # Get a set of coefficients and error\n",
    "        w = STRidge(R,Ut,lam,STR_iters,tol,normalize = normalize)\n",
    "        err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "\n",
    "        # Has the accuracy improved?\n",
    "        if err <= err_best:\n",
    "            err_best = err\n",
    "            w_best = w\n",
    "            tol_best = tol\n",
    "            tol = tol + d_tol\n",
    "\n",
    "        else:\n",
    "            tol = max([0,tol - 2*d_tol])\n",
    "            d_tol  = 2*d_tol / (maxit - iter)\n",
    "            tol = tol + d_tol\n",
    "\n",
    "    if print_best_tol: \n",
    "        print (\"Optimal tolerance:\", tol_best)\n",
    "    return w_best\n",
    "\n",
    "def STRidge(X0, y, lam, maxit, tol, normalize = 2, print_results = False):\n",
    "    \"\"\"\n",
    "    Sequential Threshold Ridge Regression algorithm for finding (hopefully) sparse \n",
    "    approximation to X^{-1}y.  The idea is that this may do better with correlated observables.\n",
    "\n",
    "    This assumes y is only one column\n",
    "    \"\"\"\n",
    "\n",
    "    n,d = X0.shape\n",
    "    X = np.zeros((n,d), dtype=np.complex64)\n",
    "    # First normalize data\n",
    "    if normalize != 0:\n",
    "        Mreg = np.zeros((d,1))\n",
    "        for i in range(0,d):\n",
    "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
    "            X[:,i] = Mreg[i]*X0[:,i]\n",
    "    else: X = X0\n",
    "    \n",
    "    # Get the standard ridge esitmate\n",
    "    if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y))[0]\n",
    "    else: w = np.linalg.lstsq(X,y)[0]\n",
    "    num_relevant = d\n",
    "    biginds = np.where( abs(w) > tol)[0]\n",
    "    \n",
    "    # Threshold and continue\n",
    "    for j in range(maxit):\n",
    "\n",
    "        # Figure out which items to cut out\n",
    "        smallinds = np.where( abs(w) < tol)[0]\n",
    "        new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "            \n",
    "        # If nothing changes then stop\n",
    "        if num_relevant == len(new_biginds): break\n",
    "        else: num_relevant = len(new_biginds)\n",
    "            \n",
    "        # Also make sure we didn't just lose all the coefficients\n",
    "        if len(new_biginds) == 0:\n",
    "            if j == 0: \n",
    "                #if print_results: print \"Tolerance too high - all coefficients set below tolerance\"\n",
    "                return w\n",
    "            else: break\n",
    "        biginds = new_biginds\n",
    "        \n",
    "        # Otherwise get a new guess\n",
    "        w[smallinds] = 0\n",
    "        if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y))[0]\n",
    "        else: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
    "\n",
    "    # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
    "    \n",
    "    if normalize != 0: return np.multiply(Mreg,w)\n",
    "    else: return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe2e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10 #m轮更新\n",
    "b = 1.2 #叛逃者的收益\n",
    "K = 0.1\n",
    "P_PDG = array([[1,0],[b,0]]) #2*2的收益矩阵\n",
    "Noise = 5\n",
    "W = np.loadtxt(\"WS.txt\") \n",
    "T = W.shape[0] #全部社区内个体的总数量\n",
    "SV = 6\n",
    "# the part for generate EG data\n",
    "y = zeros((SV*M,T))\n",
    "AA = zeros((T,SV*M,T))\n",
    "for i in range(SV): #产生6组A和Y的值\n",
    "    [A, Aa] = data_generate(M,P_PDG,W,T,K,Noise)\n",
    "    for j in range(M):\n",
    "        y[i*M+j,:] =Aa[j,:]\n",
    "        for k in range(T):\n",
    "            AA[k,i*M+j,:] = A[k,j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56dada7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.69265438 0.51357265 4.02661149 0.13726854 3.25126457 1.68955557\n",
      "  2.51779248 4.02904083 2.73761267 0.49340343 3.11808451 3.50380677\n",
      "  4.15783328 4.52863242 3.62601879 4.6525596  3.11131971 3.37760914\n",
      "  2.64038936 3.37980362 1.77334013 3.5709909  3.04300564 4.85706353\n",
      "  2.50626619 3.87082846 0.34902722 3.85540937 1.87395512 1.04623035\n",
      "  4.6924986  2.31740041 4.55362981 3.87329129 1.18902657 2.87442146\n",
      "  1.54317598 3.45438416 3.66687711 1.01683532]\n",
      " [3.64909392 1.59323837 3.9076025  4.65393561 2.03047172 2.49143693\n",
      "  0.63887809 3.54998096 4.08627156 3.52507317 0.04122232 1.44192856\n",
      "  4.84523167 0.7232371  4.84961178 4.28063702 4.7348764  3.53376659\n",
      "  1.15456519 1.42134236 4.77304765 3.47019412 1.75392243 2.78462327\n",
      "  2.83214859 2.31226353 1.74004106 0.79514197 0.20994003 2.33126882\n",
      "  3.01156815 1.83218623 3.68826539 4.71249564 4.11915336 4.53733129\n",
      "  2.19937301 3.85806145 4.09783868 0.05507884]\n",
      " [0.76821508 0.69398667 2.74318138 1.53160718 3.62715495 4.96680505\n",
      "  1.54884997 1.74026495 2.89698357 3.67880659 2.6047282  2.2839888\n",
      "  0.58444342 2.31069359 3.08285274 1.92448771 0.32419573 4.81247389\n",
      "  0.26005985 4.71186452 0.06444625 2.93818735 2.9572835  0.16651441\n",
      "  4.13916154 2.00251125 0.63135686 0.38206739 0.24540603 2.08460997\n",
      "  2.85726803 3.28506981 2.14988077 2.62720733 3.66641187 4.84857675\n",
      "  3.27791312 2.63649623 0.11186973 4.83701965]\n",
      " [4.77648617 3.60164539 3.69816276 1.95070792 3.29308187 3.42459953\n",
      "  1.56776146 1.30736427 0.30621756 3.05307816 4.21770097 3.06747634\n",
      "  2.70307314 1.40337757 1.16763481 3.4755089  3.65598781 4.81515712\n",
      "  0.25036784 4.75203864 3.22097264 1.89270124 1.14736645 0.66323066\n",
      "  0.37453089 4.65529116 4.98184245 0.06914574 2.41629955 2.76912426\n",
      "  1.08350892 4.89766318 3.68441786 0.91557127 0.65077258 3.43501663\n",
      "  2.20358138 2.93429086 0.69409749 1.59121942]\n",
      " [0.36807335 2.68164981 3.90985245 4.75063761 1.77982798 2.30216564\n",
      "  4.62538355 4.35213892 2.64477388 2.14732851 2.66314588 0.81569455\n",
      "  3.89315769 0.70793378 3.33323726 0.76352881 4.81098841 2.25870336\n",
      "  4.14434208 2.25242897 4.10119701 3.14847664 1.64016461 0.29660726\n",
      "  0.27037349 4.06124122 2.7628444  4.07277151 4.61316039 3.87274178\n",
      "  3.9720072  2.62065564 1.77152184 3.31207133 3.61008224 3.5650147\n",
      "  1.5841382  3.94893196 4.16283613 3.02825867]\n",
      " [4.40194203 2.16358553 3.54955903 1.08098596 0.56100865 4.56360207\n",
      "  0.89359969 2.86110839 1.17672027 2.58883525 0.38735047 4.33438425\n",
      "  1.97374956 0.77144954 3.63799554 0.13724436 3.4464256  3.2211982\n",
      "  3.07390421 4.89263007 1.46374493 2.04399835 2.41994956 3.63879653\n",
      "  3.72922043 2.29015177 2.74653772 2.87338477 2.61617997 4.05201834\n",
      "  3.15341195 2.50690767 3.05906047 4.99526581 0.95374158 2.30451244\n",
      "  1.86504487 0.26036402 2.50839099 3.62429571]\n",
      " [2.69569691 2.340696   0.5241789  4.19631699 0.44216294 1.65325358\n",
      "  1.38278016 3.73887484 4.87202861 1.2014705  1.80261307 1.41548827\n",
      "  3.42840996 1.9939586  4.33797226 2.55165042 1.23212327 1.94096735\n",
      "  2.89224024 4.6741477  4.9938597  2.36687158 1.77090887 1.70614973\n",
      "  4.60714464 0.27350801 4.29036383 0.14080064 3.44001267 0.78296515\n",
      "  2.04166528 0.60153232 0.88033209 4.19499011 0.00542572 0.84880143\n",
      "  3.03712502 0.90157713 3.84554561 0.46554066]\n",
      " [0.2722685  3.02421918 2.60613871 1.90834574 2.47351233 2.04687501\n",
      "  1.0733184  4.44908959 1.21643183 3.72732993 3.59200687 0.94713613\n",
      "  0.39650808 4.74710826 0.93846276 4.10935889 4.53165118 2.59632381\n",
      "  3.36367731 4.11638176 3.17123271 2.08834499 3.74213824 0.81808124\n",
      "  0.69250714 3.99363066 1.56888804 2.19037608 4.47678483 2.5664468\n",
      "  2.77933677 4.84416877 2.52795111 4.63563437 4.89881365 4.03356743\n",
      "  2.0995134  3.77721101 4.01843008 2.60329978]\n",
      " [2.27876698 4.38568863 1.44989433 1.00709323 2.69364588 3.15732382\n",
      "  0.35810848 3.80446947 1.54655577 1.67838163 4.21100185 2.25351639\n",
      "  3.96189287 3.01014622 0.24641088 3.29681741 2.2247321  0.15902724\n",
      "  0.0319589  3.79520119 1.50908337 4.69516928 4.31001833 0.42494517\n",
      "  0.56127507 1.81057214 0.76681514 0.65918998 2.31958122 0.98566228\n",
      "  0.8303725  0.41120818 2.04493189 3.54292269 4.99083413 1.87692202\n",
      "  1.81406225 1.69997043 0.74856039 0.0728384 ]\n",
      " [0.61557927 0.87182043 2.49523929 2.73718215 2.76333701 0.03367433\n",
      "  3.12794273 2.36327321 0.95471855 0.39248543 3.68130697 2.18850983\n",
      "  2.57764454 1.73363626 0.85714803 2.74735462 3.64405054 2.04074381\n",
      "  2.65584395 0.92491421 3.04623622 0.31329496 1.44404586 0.36900072\n",
      "  0.3037706  4.72309831 0.73469119 4.23225334 3.30484566 2.51676269\n",
      "  3.03747096 2.48989198 3.56680808 0.61130093 2.38157191 3.69934221\n",
      "  3.10316717 2.81090209 0.27730731 1.4075323 ]]\n"
     ]
    }
   ],
   "source": [
    "print(Noise*random.random((M,T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf2aa481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-13ac3e2d7f51>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-3-13ac3e2d7f51>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y))[0]\n",
      "<ipython-input-11-0f2aa95661a7>:5: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "for j in range(T):\n",
    "    w = TrainSTRidge(AA[j], y[:,j].reshape(-1,1),10**2,1)\n",
    "    for i in range(w.shape[0]):\n",
    "        X[i,j]=w[i,0]\n",
    "np.savetxt(\"X.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b210bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge_SPL(R, Ut, v, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "    \"\"\"\n",
    "    This function trains a predictor using STRidge_SPL.\n",
    "\n",
    "    It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "    using a loss function on a holdout set.\n",
    "\n",
    "    Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "    not squared 2-norm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "    np.random.seed(0) # for consistancy\n",
    "    n,_ = R.shape\n",
    "    train = np.random.choice(n, int(n*split), replace = False)\n",
    "    test = [i for i in np.arange(n) if i not in train]\n",
    "    TrainR = R[train,:]\n",
    "    TestR = R[test,:]\n",
    "    TrainY = Ut[train,:]\n",
    "    TestY = Ut[test,:]\n",
    "    D = TrainR.shape[1]       \n",
    "\n",
    "    # Set up the initial tolerance and l0 penalty\n",
    "    d_tol = float(d_tol)\n",
    "    tol = d_tol\n",
    "    if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
    "\n",
    "    # Get the standard least squares estimator\n",
    "    w = np.zeros((D,1))\n",
    "    w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
    "    err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "    tol_best = 0\n",
    "\n",
    "    # Now increase tolerance until test performance decreases\n",
    "    for iter in range(maxit):\n",
    "\n",
    "        # Get a set of coefficients and error\n",
    "        w = STRidge_SPL(R,Ut,v,lam,STR_iters,tol,normalize = normalize)\n",
    "        err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "\n",
    "        # Has the accuracy improved?\n",
    "        if err <= err_best:\n",
    "            err_best = err\n",
    "            w_best = w\n",
    "            tol_best = tol\n",
    "            tol = tol + d_tol\n",
    "\n",
    "        else:\n",
    "            tol = max([0,tol - 2*d_tol])\n",
    "            d_tol  = 2*d_tol / (maxit - iter)\n",
    "            tol = tol + d_tol\n",
    "\n",
    "    if print_best_tol: \n",
    "        print (\"Optimal tolerance:\", tol_best)\n",
    "    return w_best\n",
    "\n",
    "def STRidge_SPL(X0, y, v, lam, maxit, tol, normalize = 2, print_results = False):\n",
    "    \"\"\"\n",
    "    Sequential Threshold Ridge Regression algorithm for finding (hopefully) sparse \n",
    "    approximation to X^{-1}y.  The idea is that this may do better with correlated observables.\n",
    "\n",
    "    This assumes y is only one column\n",
    "    \"\"\"\n",
    "\n",
    "    n,d = X0.shape\n",
    "    X = np.zeros((n,d), dtype=np.complex64)\n",
    "    # First normalize data\n",
    "    if normalize != 0:\n",
    "        Mreg = np.zeros((d,1))\n",
    "        for i in range(0,d):\n",
    "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
    "            X[:,i] = Mreg[i]*X0[:,i]\n",
    "    else: X = X0\n",
    "    \n",
    "    # Get the standard ridge esitmate\n",
    "    if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
    "    else: w = np.linalg.lstsq(X*v,y*v)[0]\n",
    "    num_relevant = d\n",
    "    biginds = np.where( abs(w) > tol)[0]\n",
    "    \n",
    "    # Threshold and continue\n",
    "    for j in range(maxit):\n",
    "\n",
    "        # Figure out which items to cut out\n",
    "        smallinds = np.where( abs(w) < tol)[0]\n",
    "        new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "            \n",
    "        # If nothing changes then stop\n",
    "        if num_relevant == len(new_biginds): break\n",
    "        else: num_relevant = len(new_biginds)\n",
    "            \n",
    "        # Also make sure we didn't just lose all the coefficients\n",
    "        if len(new_biginds) == 0:\n",
    "            if j == 0: \n",
    "                #if print_results: print \"Tolerance too high - all coefficients set below tolerance\"\n",
    "                return w\n",
    "            else: break\n",
    "        biginds = new_biginds\n",
    "        \n",
    "        # Otherwise get a new guess\n",
    "        w[smallinds] = 0\n",
    "        if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n",
    "        else: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
    "\n",
    "    # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
    "    \n",
    "    if normalize != 0: return np.multiply(Mreg,w)\n",
    "    else: return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d6f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-5e04d6e95ebc>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-12-5e04d6e95ebc>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
      "<ipython-input-16-d030a23c8e10>:18: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "v = ones((SV*M,1))\n",
    "v1 = zeros((SV*M,1))\n",
    "Los = zeros((SV*M,T))\n",
    "losss = zeros((SV*M,1))\n",
    "lambda_0=2\n",
    "t=0\n",
    "while (t<16):  \n",
    "    if (v==v1).all():\n",
    "        break\n",
    "    else:\n",
    "        for j in range(T):\n",
    "            w = TrainSTRidge_SPL(AA[j], y[:,j].reshape(-1,1), v,10**2,1)\n",
    "            Lo =  abs((np.dot(AA[j], w))-y[:,j].reshape(-1,1))\n",
    "            for i in range(SV*M):\n",
    "                Los[i,j]=Lo[i]\n",
    "            for i in range(w.shape[0]):\n",
    "                X[i,j]=w[i,0]\n",
    "        for i in range(SV*M):\n",
    "            losss[i]=np.mean(Los[i,:])\n",
    "        t = t+1\n",
    "        for i in range(SV*M):\n",
    "            if losss[i]<lambda_0:\n",
    "                v[i]=1-losss[i]/lambda_0\n",
    "            else:\n",
    "                v[i]=0\n",
    "        lambda_0 = lambda_0*1.5\n",
    "        #lambda_0 = lambda_0 + np.mean(losss)      \n",
    "np.savetxt(\"X1.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff62e5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99830115],\n",
       "       [0.99812428],\n",
       "       [0.99811838],\n",
       "       [0.99779357],\n",
       "       [0.99746899],\n",
       "       [0.99770018],\n",
       "       [0.99719328],\n",
       "       [0.99740594],\n",
       "       [0.99717891],\n",
       "       [0.99710611],\n",
       "       [0.99799583],\n",
       "       [0.99774702],\n",
       "       [0.99729906],\n",
       "       [0.99787312],\n",
       "       [0.99754951],\n",
       "       [0.99683237],\n",
       "       [0.99712679],\n",
       "       [0.99747926],\n",
       "       [0.9971028 ],\n",
       "       [0.9971342 ],\n",
       "       [0.99771579],\n",
       "       [0.99729782],\n",
       "       [0.99681031],\n",
       "       [0.99721149],\n",
       "       [0.99692408],\n",
       "       [0.99743334],\n",
       "       [0.99717954],\n",
       "       [0.99680309],\n",
       "       [0.99710985],\n",
       "       [0.99749965],\n",
       "       [0.99792936],\n",
       "       [0.99823314],\n",
       "       [0.99766434],\n",
       "       [0.99790472],\n",
       "       [0.99835304],\n",
       "       [0.99813492],\n",
       "       [0.99778886],\n",
       "       [0.99788805],\n",
       "       [0.99764263],\n",
       "       [0.99731239],\n",
       "       [0.99763297],\n",
       "       [0.99776047],\n",
       "       [0.99781235],\n",
       "       [0.99799416],\n",
       "       [0.99777265],\n",
       "       [0.99775174],\n",
       "       [0.99771426],\n",
       "       [0.99742051],\n",
       "       [0.99741157],\n",
       "       [0.99822378],\n",
       "       [0.99772828],\n",
       "       [0.99763019],\n",
       "       [0.99790962],\n",
       "       [0.99758395],\n",
       "       [0.99764463],\n",
       "       [0.99755523],\n",
       "       [0.9975852 ],\n",
       "       [0.99735599],\n",
       "       [0.99764552],\n",
       "       [0.99778811]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e00da151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-5e04d6e95ebc>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-12-5e04d6e95ebc>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
      "<ipython-input-25-9e11dcd4c295>:17: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "v = ones((SV*M,T))\n",
    "v1 = zeros((SV*M,T))\n",
    "Los = zeros((SV*M,T))\n",
    "lambda_0=3\n",
    "t=0\n",
    "while (t<30):\n",
    "    if (v==v1).all():\n",
    "        break\n",
    "    else:\n",
    "        for j in range(T):\n",
    "            w = TrainSTRidge_SPL(AA[j], y[:,j].reshape(-1,1), v[:,j].reshape(-1,1),10**2,1)\n",
    "            Lo =  abs((np.dot(AA[j], w))-y[:,j].reshape(-1,1))\n",
    "            for i in range(SV*M):\n",
    "                Los[i,j]=Lo[i]\n",
    "            for i in range(w.shape[0]):\n",
    "                X[i,j]=w[i,0]\n",
    "        t = t+1\n",
    "        for j in range(T):\n",
    "            for i in range(SV*M):\n",
    "                if Los[i,j]<lambda_0:\n",
    "                    v[i,j]=1-Los[i,j]/lambda_0\n",
    "                else:\n",
    "                    v[i,j]=0\n",
    "        lambda_0 = lambda_0*1.25\n",
    "        #lambda_0 = lambda_0 + np.mean(Los)\n",
    "np.savetxt(\"X2.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15d2e7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9993894 , 0.99966671, 0.99966877, ..., 0.99920064, 0.9979708 ,\n",
       "        0.99871612],\n",
       "       [0.99843102, 0.99999964, 0.99877664, ..., 0.99785543, 0.99897967,\n",
       "        0.99975676],\n",
       "       [0.99998346, 0.99943058, 0.99959216, ..., 0.99771721, 0.99789302,\n",
       "        0.99918457],\n",
       "       ...,\n",
       "       [0.99944716, 0.99993526, 0.99837775, ..., 0.99841655, 0.99944365,\n",
       "        0.9994305 ],\n",
       "       [0.99886558, 0.99988599, 0.99956603, ..., 0.99961834, 0.99845939,\n",
       "        0.99931353],\n",
       "       [0.99956153, 0.99993893, 0.99948811, ..., 0.99840202, 0.99854812,\n",
       "        0.99999366]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86360c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
