{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2990296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "from random import shuffle\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9b6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generate(M,P_PDG,W,N,K,Noise):\n",
    "    #W是原数据集,代表哪些代理之间有连接，即网络\n",
    "    #N是W的行数，即代理的个数\n",
    "    #代理人的博弈策略\n",
    "    player = zeros((M+1,N))\n",
    "    #原始博弈策略放在第一行，随机设置\n",
    "    for i in range(N):\n",
    "        if random.random()<=0.5:\n",
    "            player[0,i] = 1 #1代表合作\n",
    "        else:\n",
    "            player[0,i] = 0 #0代表不合作\n",
    "    #计算每个节点的收益\n",
    "    #M轮博弈\n",
    "    F = zeros((1,N))\n",
    "    G = zeros((M,N))\n",
    "    A = zeros((N,M,N))\n",
    "    for t in range(M):\n",
    "        for i in range(N):\n",
    "            if player[t,i] == 1:\n",
    "                s1 = array([[1],[0]])\n",
    "            else:\n",
    "                s1 = array([[0],[1]])\n",
    "            for j in range(N):\n",
    "                if player[t,j] == 0:\n",
    "                    s2 = array([[0],[1]])\n",
    "                else:\n",
    "                    s2 = array([[1],[0]])\n",
    "                F[0,j] = ((s1.T).dot(P_PDG)).dot(s2) #如果代理i和j连接，则代理i的收益为F[0,j]\n",
    "            A[i,t,:] = F  #A:N*M*N   \n",
    "            # F是三维矩阵A的第i页，第t行\n",
    "            G[t,i] = F.dot(W[:,i]) #第t轮代理i的收益\n",
    "        # update strategies\n",
    "        for k in range(N):\n",
    "            s=[i for i,x in enumerate(list(W[:,k])) if x>=1]     # 找出与代理k合作的代理的索引\n",
    "            if len(s)!=0: # 如果有代理与代理k合作\n",
    "                shuffle(s)\n",
    "                P = 1/(1+math.e**((G[t,k]-G[t,s[0]])/K)) # 费米规则\n",
    "                if random.random()<= P:\n",
    "                    player[t+1,k] = player[t,s[0]]\n",
    "                else:\n",
    "                    player[t+1,k] = player[t,k]\n",
    "            else: # 如果没有代理与代理k合作\n",
    "                player[t+1,k] = player[t,k]\n",
    "    # add noise for G\n",
    "    Aa = G + Noise*random.random((M,N)) # m轮收益总矩阵：包含每一轮的收益M*N\n",
    "    return [A,Aa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40594aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge(R, Ut, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "    \"\"\"\n",
    "    This function trains a predictor using STRidge.\n",
    "\n",
    "    It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "    using a loss function on a holdout set.\n",
    "\n",
    "    Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "    not squared 2-norm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "    np.random.seed(0) # for consistancy\n",
    "    n,_ = R.shape\n",
    "    train = np.random.choice(n, int(n*split), replace = False)\n",
    "    test = [i for i in np.arange(n) if i not in train]\n",
    "    TrainR = R[train,:]\n",
    "    TestR = R[test,:]\n",
    "    TrainY = Ut[train,:]\n",
    "    TestY = Ut[test,:]\n",
    "    D = TrainR.shape[1]       \n",
    "\n",
    "    # Set up the initial tolerance and l0 penalty\n",
    "    d_tol = float(d_tol)\n",
    "    tol = d_tol\n",
    "    if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
    "\n",
    "    # Get the standard least squares estimator\n",
    "    w = np.zeros((D,1))\n",
    "    w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
    "    err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "    tol_best = 0\n",
    "\n",
    "    # Now increase tolerance until test performance decreases\n",
    "    for iter in range(maxit):\n",
    "\n",
    "        # Get a set of coefficients and error\n",
    "        w = STRidge(R,Ut,lam,STR_iters,tol,normalize = normalize)\n",
    "        err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "\n",
    "        # Has the accuracy improved?\n",
    "        if err <= err_best:\n",
    "            err_best = err\n",
    "            w_best = w\n",
    "            tol_best = tol\n",
    "            tol = tol + d_tol\n",
    "\n",
    "        else:\n",
    "            tol = max([0,tol - 2*d_tol])\n",
    "            d_tol  = 2*d_tol / (maxit - iter)\n",
    "            tol = tol + d_tol\n",
    "\n",
    "    if print_best_tol: \n",
    "        print (\"Optimal tolerance:\", tol_best)\n",
    "    return w_best\n",
    "\n",
    "def STRidge(X0, y, lam, maxit, tol, normalize = 2, print_results = False):\n",
    "    \"\"\"\n",
    "    Sequential Threshold Ridge Regression algorithm for finding (hopefully) sparse \n",
    "    approximation to X^{-1}y.  The idea is that this may do better with correlated observables.\n",
    "\n",
    "    This assumes y is only one column\n",
    "    \"\"\"\n",
    "\n",
    "    n,d = X0.shape\n",
    "    X = np.zeros((n,d), dtype=np.complex64)\n",
    "    # First normalize data\n",
    "    if normalize != 0:\n",
    "        Mreg = np.zeros((d,1))\n",
    "        for i in range(0,d):\n",
    "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
    "            X[:,i] = Mreg[i]*X0[:,i]\n",
    "    else: X = X0\n",
    "    \n",
    "    # Get the standard ridge esitmate\n",
    "    if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y))[0]\n",
    "    else: w = np.linalg.lstsq(X,y)[0]\n",
    "    num_relevant = d\n",
    "    biginds = np.where( abs(w) > tol)[0]\n",
    "    \n",
    "    # Threshold and continue\n",
    "    for j in range(maxit):\n",
    "\n",
    "        # Figure out which items to cut out\n",
    "        smallinds = np.where( abs(w) < tol)[0]\n",
    "        new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "            \n",
    "        # If nothing changes then stop\n",
    "        if num_relevant == len(new_biginds): break\n",
    "        else: num_relevant = len(new_biginds)\n",
    "            \n",
    "        # Also make sure we didn't just lose all the coefficients\n",
    "        if len(new_biginds) == 0:\n",
    "            if j == 0: \n",
    "                #if print_results: print \"Tolerance too high - all coefficients set below tolerance\"\n",
    "                return w\n",
    "            else: break\n",
    "        biginds = new_biginds\n",
    "        \n",
    "        # Otherwise get a new guess\n",
    "        w[smallinds] = 0\n",
    "        if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y))[0]\n",
    "        else: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
    "\n",
    "    # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
    "    \n",
    "    if normalize != 0: return np.multiply(Mreg,w)\n",
    "    else: return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe2e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10 #m轮更新\n",
    "b = 1.2 #叛逃者的收益\n",
    "K = 0.1\n",
    "P_PDG = array([[1,0],[b,0]]) #2*2的收益矩阵\n",
    "Noise = 5\n",
    "W = np.loadtxt(\"ER.txt\") \n",
    "T = W.shape[0] #全部社区内个体的总数量\n",
    "SV = 6\n",
    "# the part for generate EG data\n",
    "y = zeros((SV*M,T))\n",
    "AA = zeros((T,SV*M,T))\n",
    "for i in range(SV): #产生6组A和Y的值\n",
    "    [A, Aa] = data_generate(M,P_PDG,W,T,K,Noise)\n",
    "    for j in range(M):\n",
    "        y[i*M+j,:] =Aa[j,:]\n",
    "        for k in range(T):\n",
    "            AA[k,i*M+j,:] = A[k,j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab403e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.681487   1.20326564 4.89165635 1.07076296 4.49323416 2.49156494\n",
      "  1.48091186 1.43632425 2.23784116 1.7839017  3.64168004 3.83741699\n",
      "  1.04186373 4.83878563 3.11101373 0.22786344 4.40003847 3.11673068\n",
      "  2.1384704  3.40765058 1.23175884 4.44582622 0.54995869 3.28397975\n",
      "  3.75462363 3.11530824 2.39343522 0.66419048 4.98305549 3.73872666\n",
      "  2.44874463 0.37524703 0.8338022  3.49972682 2.55984609 0.06434849\n",
      "  3.34828289 2.35424758 1.37118066 3.49949608]\n",
      " [1.49514946 0.56413995 3.93349596 1.68465678 3.82629017 2.2198006\n",
      "  2.26904405 4.13404045 3.45249522 3.86514593 1.93464513 4.49526749\n",
      "  1.08244897 4.60956914 2.68551457 3.7449877  4.916005   1.95905904\n",
      "  3.25039733 0.48230999 0.87823516 3.32915228 3.98236859 0.40769645\n",
      "  2.64957147 4.24744759 4.69517863 4.53962514 3.9399584  4.3306849\n",
      "  4.72484089 4.71827546 0.33670258 2.78077988 3.11977476 3.09017523\n",
      "  4.82027627 3.44343841 1.82179695 2.46147573]\n",
      " [0.51494555 2.22870726 1.97713581 1.4832022  3.84419022 0.80180403\n",
      "  0.93337083 1.35366045 1.82979519 4.60534909 4.7332099  2.09449915\n",
      "  1.23059721 4.79925675 4.07565577 3.77610431 0.38529055 0.27634391\n",
      "  1.19757521 4.45569437 3.6180656  4.52706045 4.63783298 3.36630519\n",
      "  0.76440182 3.26380665 3.45273602 4.97597267 1.70595687 0.88937841\n",
      "  2.92189065 4.17198308 2.44680435 4.57801865 2.46789415 3.65696955\n",
      "  4.70693817 1.39939508 1.0016473  2.79149838]\n",
      " [1.44289196 0.44848427 3.63164783 0.16004855 1.03984056 4.76895914\n",
      "  1.8573893  1.27949217 2.33790158 0.02779678 4.35287436 0.35397499\n",
      "  0.67408844 0.71796341 1.75216222 2.84513542 0.89357438 0.5368235\n",
      "  4.80548219 3.32117164 4.87161308 1.33577684 4.14703566 4.05081686\n",
      "  4.73166198 2.05849326 2.4352459  3.24563959 2.4537912  4.50614635\n",
      "  0.08926332 1.28927845 1.9058746  0.17699414 1.08814174 2.7181453\n",
      "  2.66855639 1.50114801 4.11794555 4.29104374]\n",
      " [1.13574476 3.13031994 3.93546763 4.49240024 3.13348468 1.03234303\n",
      "  4.4121354  4.12818869 0.41430936 0.871472   2.76881727 3.66294339\n",
      "  0.07577608 1.92124622 4.22669398 1.29893323 1.18084281 0.70357537\n",
      "  1.4900027  4.36944376 4.08302089 3.30283275 2.21905263 1.20234577\n",
      "  3.93614457 2.478098   1.91442841 0.34489094 1.66334138 0.33660091\n",
      "  1.74398117 2.12944836 3.28833225 4.30383004 2.95905549 0.02205677\n",
      "  4.11189999 2.9421727  3.63626112 1.67544929]\n",
      " [4.38833748 1.17632858 4.5893995  1.40927218 4.76711456 3.92069333\n",
      "  2.68503548 1.38662935 3.69963304 4.40199016 4.76970823 2.20000354\n",
      "  0.70441149 2.06899556 4.80666329 0.78909503 4.67247897 1.15286196\n",
      "  4.81440795 2.77664182 0.52443952 4.25576036 3.77756877 0.22485507\n",
      "  2.03878538 2.40454022 0.81562593 1.74393193 2.51785913 1.54852637\n",
      "  1.98133717 0.6415688  4.74571136 4.579387   4.77245342 3.27937698\n",
      "  0.27280262 2.98467121 4.76036426 0.73952999]\n",
      " [2.55572622 1.84249898 0.48805724 0.47191787 2.49953961 3.1088328\n",
      "  1.51508766 1.90196354 1.49428294 2.00781586 2.79545022 3.92213326\n",
      "  4.15815431 2.30009578 3.36946222 0.2810472  4.4749234  3.1226544\n",
      "  2.0531252  4.53641577 4.58557057 2.4473177  3.60106778 2.31303857\n",
      "  3.34608238 2.62950736 2.20464086 0.14541839 2.37863885 0.53600849\n",
      "  3.75382633 0.3986768  0.89539866 1.02561156 2.17982772 1.80177249\n",
      "  1.02354207 4.73360813 1.06122896 0.27912432]\n",
      " [0.67276108 0.5163472  1.27494219 4.98404333 4.42662456 3.14200994\n",
      "  1.85827996 0.37006579 1.9061152  4.26018018 2.13115165 0.88561951\n",
      "  0.80596169 4.4614305  2.9802152  0.29307225 0.43113095 1.98811449\n",
      "  0.75142189 2.2232103  1.03028825 0.06969303 1.50916738 0.25295741\n",
      "  0.77847495 3.49504305 3.7091352  3.69877958 1.39931742 2.51541963\n",
      "  1.91951861 0.19012259 3.46545845 1.60586054 1.55748065 2.45196072\n",
      "  3.43372424 2.75306189 1.36154387 1.613448  ]\n",
      " [1.68327079 0.68812836 3.9033759  0.37093858 3.99906475 4.27279175\n",
      "  2.76301132 1.43335755 2.07649674 3.05657729 3.40021201 3.43596095\n",
      "  4.79670702 0.47614764 1.46012762 1.8145476  1.92002507 2.24881155\n",
      "  3.27666407 0.34753707 2.42040665 1.37019768 1.14255646 4.99821933\n",
      "  3.4547352  3.84510204 2.78469277 0.4004981  2.7769128  1.1140014\n",
      "  4.72053511 2.21550528 4.6044286  0.89329164 3.58053322 0.54124161\n",
      "  1.08512423 4.58615401 4.2732376  4.88171334]\n",
      " [2.13145948 1.1468451  3.9061894  4.3236024  2.21254826 0.73891136\n",
      "  1.77214207 4.89872725 0.38221248 0.26946252 3.86477253 4.49628318\n",
      "  4.34747991 0.106677   2.15714457 3.22020596 2.82249052 2.89416295\n",
      "  2.15147594 4.49192649 3.51086535 4.89932318 0.81244294 1.90109261\n",
      "  0.91196896 0.86584719 0.3358041  3.8392718  0.56242346 4.29008193\n",
      "  3.2935169  3.16844238 2.77009814 4.6021277  0.96647936 1.56134001\n",
      "  0.94662875 0.42657624 0.9945148  2.99396056]]\n"
     ]
    }
   ],
   "source": [
    "print(Noise*random.random((M,T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf2aa481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-13ac3e2d7f51>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-3-13ac3e2d7f51>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y))[0]\n",
      "<ipython-input-11-0f2aa95661a7>:5: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "for j in range(T):\n",
    "    w = TrainSTRidge(AA[j], y[:,j].reshape(-1,1),10**2,1)\n",
    "    for i in range(w.shape[0]):\n",
    "        X[i,j]=w[i,0]\n",
    "np.savetxt(\"X.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b210bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge_SPL(R, Ut, v, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "    \"\"\"\n",
    "    This function trains a predictor using STRidge_SPL.\n",
    "\n",
    "    It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "    using a loss function on a holdout set.\n",
    "\n",
    "    Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "    not squared 2-norm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "    np.random.seed(0) # for consistancy\n",
    "    n,_ = R.shape\n",
    "    train = np.random.choice(n, int(n*split), replace = False)\n",
    "    test = [i for i in np.arange(n) if i not in train]\n",
    "    TrainR = R[train,:]\n",
    "    TestR = R[test,:]\n",
    "    TrainY = Ut[train,:]\n",
    "    TestY = Ut[test,:]\n",
    "    D = TrainR.shape[1]       \n",
    "\n",
    "    # Set up the initial tolerance and l0 penalty\n",
    "    d_tol = float(d_tol)\n",
    "    tol = d_tol\n",
    "    if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
    "\n",
    "    # Get the standard least squares estimator\n",
    "    w = np.zeros((D,1))\n",
    "    w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
    "    err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "    tol_best = 0\n",
    "\n",
    "    # Now increase tolerance until test performance decreases\n",
    "    for iter in range(maxit):\n",
    "\n",
    "        # Get a set of coefficients and error\n",
    "        w = STRidge_SPL(R,Ut,v,lam,STR_iters,tol,normalize = normalize)\n",
    "        err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "\n",
    "        # Has the accuracy improved?\n",
    "        if err <= err_best:\n",
    "            err_best = err\n",
    "            w_best = w\n",
    "            tol_best = tol\n",
    "            tol = tol + d_tol\n",
    "\n",
    "        else:\n",
    "            tol = max([0,tol - 2*d_tol])\n",
    "            d_tol  = 2*d_tol / (maxit - iter)\n",
    "            tol = tol + d_tol\n",
    "\n",
    "    if print_best_tol: \n",
    "        print (\"Optimal tolerance:\", tol_best)\n",
    "    return w_best\n",
    "\n",
    "def STRidge_SPL(X0, y, v, lam, maxit, tol, normalize = 2, print_results = False):\n",
    "    \"\"\"\n",
    "    Sequential Threshold Ridge Regression algorithm for finding (hopefully) sparse \n",
    "    approximation to X^{-1}y.  The idea is that this may do better with correlated observables.\n",
    "\n",
    "    This assumes y is only one column\n",
    "    \"\"\"\n",
    "\n",
    "    n,d = X0.shape\n",
    "    X = np.zeros((n,d), dtype=np.complex64)\n",
    "    # First normalize data\n",
    "    if normalize != 0:\n",
    "        Mreg = np.zeros((d,1))\n",
    "        for i in range(0,d):\n",
    "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
    "            X[:,i] = Mreg[i]*X0[:,i]\n",
    "    else: X = X0\n",
    "    \n",
    "    # Get the standard ridge esitmate\n",
    "    if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
    "    else: w = np.linalg.lstsq(X*v,y*v)[0]\n",
    "    num_relevant = d\n",
    "    biginds = np.where( abs(w) > tol)[0]\n",
    "    \n",
    "    # Threshold and continue\n",
    "    for j in range(maxit):\n",
    "\n",
    "        # Figure out which items to cut out\n",
    "        smallinds = np.where( abs(w) < tol)[0]\n",
    "        new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "            \n",
    "        # If nothing changes then stop\n",
    "        if num_relevant == len(new_biginds): break\n",
    "        else: num_relevant = len(new_biginds)\n",
    "            \n",
    "        # Also make sure we didn't just lose all the coefficients\n",
    "        if len(new_biginds) == 0:\n",
    "            if j == 0: \n",
    "                #if print_results: print \"Tolerance too high - all coefficients set below tolerance\"\n",
    "                return w\n",
    "            else: break\n",
    "        biginds = new_biginds\n",
    "        \n",
    "        # Otherwise get a new guess\n",
    "        w[smallinds] = 0\n",
    "        if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n",
    "        else: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
    "\n",
    "    # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
    "    \n",
    "    if normalize != 0: return np.multiply(Mreg,w)\n",
    "    else: return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4d6f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-5e04d6e95ebc>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-12-5e04d6e95ebc>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
      "<ipython-input-22-5346f0e2178d>:18: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n",
      "<ipython-input-12-5e04d6e95ebc>:106: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
      "<ipython-input-12-5e04d6e95ebc>:106: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
      "<ipython-input-12-5e04d6e95ebc>:102: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "v = ones((SV*M,1))\n",
    "v1 = zeros((SV*M,1))\n",
    "Los = zeros((SV*M,T))\n",
    "losss = zeros((SV*M,1))\n",
    "lambda_0=2\n",
    "t=0\n",
    "while (t<30):  \n",
    "    if (v==v1).all():\n",
    "        break\n",
    "    else:\n",
    "        for j in range(T):\n",
    "            w = TrainSTRidge_SPL(AA[j], y[:,j].reshape(-1,1), v,10**2,1)\n",
    "            Lo =  abs((np.dot(AA[j], w))-y[:,j].reshape(-1,1))\n",
    "            for i in range(SV*M):\n",
    "                Los[i,j]=Lo[i]\n",
    "            for i in range(w.shape[0]):\n",
    "                X[i,j]=w[i,0]\n",
    "        for i in range(SV*M):\n",
    "            losss[i]=np.mean(Los[i,:])\n",
    "        t = t+1\n",
    "        for i in range(SV*M):\n",
    "            if losss[i]<lambda_0:\n",
    "                v[i]=1-losss[i]/lambda_0\n",
    "            else:\n",
    "                v[i]=0\n",
    "        lambda_0 = lambda_0*1.25\n",
    "        #lambda_0 = lambda_0 + np.mean(losss)      \n",
    "np.savetxt(\"X1.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff62e5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99868311],\n",
       "       [0.99848745],\n",
       "       [0.99804135],\n",
       "       [0.99845862],\n",
       "       [0.99849062],\n",
       "       [0.99824266],\n",
       "       [0.99804631],\n",
       "       [0.99826864],\n",
       "       [0.99831466],\n",
       "       [0.99804977],\n",
       "       [0.99869315],\n",
       "       [0.99867896],\n",
       "       [0.9988304 ],\n",
       "       [0.99890624],\n",
       "       [0.99896009],\n",
       "       [0.99871795],\n",
       "       [0.99870427],\n",
       "       [0.99873978],\n",
       "       [0.99886688],\n",
       "       [0.99893247],\n",
       "       [0.99868255],\n",
       "       [0.99870671],\n",
       "       [0.99858693],\n",
       "       [0.9985291 ],\n",
       "       [0.99854637],\n",
       "       [0.99829401],\n",
       "       [0.99816122],\n",
       "       [0.99829277],\n",
       "       [0.99828893],\n",
       "       [0.99811971],\n",
       "       [0.99883226],\n",
       "       [0.99888524],\n",
       "       [0.99879034],\n",
       "       [0.99886912],\n",
       "       [0.99872797],\n",
       "       [0.99862669],\n",
       "       [0.99879409],\n",
       "       [0.99866863],\n",
       "       [0.99885913],\n",
       "       [0.99882647],\n",
       "       [0.99879473],\n",
       "       [0.99829137],\n",
       "       [0.9982494 ],\n",
       "       [0.99812035],\n",
       "       [0.99834625],\n",
       "       [0.99827703],\n",
       "       [0.99809093],\n",
       "       [0.99803668],\n",
       "       [0.99767465],\n",
       "       [0.99834768],\n",
       "       [0.9988543 ],\n",
       "       [0.99858025],\n",
       "       [0.99818096],\n",
       "       [0.9979207 ],\n",
       "       [0.99778916],\n",
       "       [0.99813325],\n",
       "       [0.99802546],\n",
       "       [0.99840216],\n",
       "       [0.99795312],\n",
       "       [0.99791007]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e00da151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-5e04d6e95ebc>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-12-5e04d6e95ebc>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
      "<ipython-input-27-196d2b896114>:17: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n",
      "<ipython-input-12-5e04d6e95ebc>:106: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
      "<ipython-input-12-5e04d6e95ebc>:106: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
      "<ipython-input-12-5e04d6e95ebc>:102: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "v = ones((SV*M,T))\n",
    "v1 = zeros((SV*M,T))\n",
    "Los = zeros((SV*M,T))\n",
    "lambda_0=2\n",
    "t=0\n",
    "while (t<30):\n",
    "    if (v==v1).all():\n",
    "        break\n",
    "    else:\n",
    "        for j in range(T):\n",
    "            w = TrainSTRidge_SPL(AA[j], y[:,j].reshape(-1,1), v[:,j].reshape(-1,1),10**2,1)\n",
    "            Lo =  abs((np.dot(AA[j], w))-y[:,j].reshape(-1,1))\n",
    "            for i in range(SV*M):\n",
    "                Los[i,j]=Lo[i]\n",
    "            for i in range(w.shape[0]):\n",
    "                X[i,j]=w[i,0]\n",
    "        t = t+1\n",
    "        for j in range(T):\n",
    "            for i in range(SV*M):\n",
    "                if Los[i,j]<lambda_0:\n",
    "                    v[i,j]=1-Los[i,j]/lambda_0\n",
    "                else:\n",
    "                    v[i,j]=0\n",
    "        lambda_0 = lambda_0*1.25\n",
    "        #lambda_0 = lambda_0 + np.mean(Los)\n",
    "np.savetxt(\"X2.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15d2e7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99690268, 0.99932899, 0.99962313, ..., 0.99937789, 0.99709882,\n",
       "        0.99936387],\n",
       "       [0.9983163 , 0.99886318, 0.99980639, ..., 0.99869197, 0.99690837,\n",
       "        0.99997211],\n",
       "       [0.99736186, 0.99767974, 0.9989742 , ..., 0.99969805, 0.99703282,\n",
       "        0.99891204],\n",
       "       ...,\n",
       "       [0.99945531, 0.99687768, 0.99900857, ..., 0.99737693, 0.99802354,\n",
       "        0.99852656],\n",
       "       [0.99932383, 0.99902089, 0.99619765, ..., 0.99849552, 0.99935326,\n",
       "        0.99741349],\n",
       "       [0.99782077, 0.99893838, 0.99634069, ..., 0.99633437, 0.99904167,\n",
       "        0.9994058 ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86360c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
