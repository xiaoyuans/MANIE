{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2990296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "from random import shuffle\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9b6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generate(M,P_PDG,W,N,K,Noise):\n",
    "    #W是原数据集,代表哪些代理之间有连接，即网络\n",
    "    #N是W的行数，即代理的个数\n",
    "    #代理人的博弈策略\n",
    "    player = zeros((M+1,N))\n",
    "    #原始博弈策略放在第一行，随机设置\n",
    "    for i in range(N):\n",
    "        if random.random()<=0.5:\n",
    "            player[0,i] = 1 #1代表合作\n",
    "        else:\n",
    "            player[0,i] = 0 #0代表不合作\n",
    "    #计算每个节点的收益\n",
    "    #M轮博弈\n",
    "    F = zeros((1,N))\n",
    "    G = zeros((M,N))\n",
    "    A = zeros((N,M,N))\n",
    "    for t in range(M):\n",
    "        for i in range(N):\n",
    "            if player[t,i] == 1:\n",
    "                s1 = array([[1],[0]])\n",
    "            else:\n",
    "                s1 = array([[0],[1]])\n",
    "            for j in range(N):\n",
    "                if player[t,j] == 0:\n",
    "                    s2 = array([[0],[1]])\n",
    "                else:\n",
    "                    s2 = array([[1],[0]])\n",
    "                F[0,j] = ((s1.T).dot(P_PDG)).dot(s2) #如果代理i和j连接，则代理i的收益为F[0,j]\n",
    "            A[i,t,:] = F  #A:N*M*N   \n",
    "            # F是三维矩阵A的第i页，第t行\n",
    "            G[t,i] = F.dot(W[:,i]) #第t轮代理i的收益\n",
    "        # update strategies\n",
    "        for k in range(N):\n",
    "            s=[i for i,x in enumerate(list(W[:,k])) if x>=1]     # 找出与代理k合作的代理的索引\n",
    "            if len(s)!=0: # 如果有代理与代理k合作\n",
    "                shuffle(s)\n",
    "                P = 1/(1+math.e**((G[t,k]-G[t,s[0]])/K)) # 费米规则\n",
    "                if random.random()<= P:\n",
    "                    player[t+1,k] = player[t,s[0]]\n",
    "                else:\n",
    "                    player[t+1,k] = player[t,k]\n",
    "            else: # 如果没有代理与代理k合作\n",
    "                player[t+1,k] = player[t,k]\n",
    "    # add noise for G\n",
    "    Aa = G + Noise*random.random((M,N)) # m轮收益总矩阵：包含每一轮的收益M*N\n",
    "    return [A,Aa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40594aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge(R, Ut, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "    \"\"\"\n",
    "    This function trains a predictor using STRidge.\n",
    "\n",
    "    It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "    using a loss function on a holdout set.\n",
    "\n",
    "    Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "    not squared 2-norm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "    np.random.seed(0) # for consistancy\n",
    "    n,_ = R.shape\n",
    "    train = np.random.choice(n, int(n*split), replace = False)\n",
    "    test = [i for i in np.arange(n) if i not in train]\n",
    "    TrainR = R[train,:]\n",
    "    TestR = R[test,:]\n",
    "    TrainY = Ut[train,:]\n",
    "    TestY = Ut[test,:]\n",
    "    D = TrainR.shape[1]       \n",
    "\n",
    "    # Set up the initial tolerance and l0 penalty\n",
    "    d_tol = float(d_tol)\n",
    "    tol = d_tol\n",
    "    if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
    "\n",
    "    # Get the standard least squares estimator\n",
    "    w = np.zeros((D,1))\n",
    "    w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
    "    err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "    tol_best = 0\n",
    "\n",
    "    # Now increase tolerance until test performance decreases\n",
    "    for iter in range(maxit):\n",
    "\n",
    "        # Get a set of coefficients and error\n",
    "        w = STRidge(R,Ut,lam,STR_iters,tol,normalize = normalize)\n",
    "        err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "\n",
    "        # Has the accuracy improved?\n",
    "        if err <= err_best:\n",
    "            err_best = err\n",
    "            w_best = w\n",
    "            tol_best = tol\n",
    "            tol = tol + d_tol\n",
    "\n",
    "        else:\n",
    "            tol = max([0,tol - 2*d_tol])\n",
    "            d_tol  = 2*d_tol / (maxit - iter)\n",
    "            tol = tol + d_tol\n",
    "\n",
    "    if print_best_tol: \n",
    "        print (\"Optimal tolerance:\", tol_best)\n",
    "    return w_best\n",
    "\n",
    "def STRidge(X0, y, lam, maxit, tol, normalize = 2, print_results = False):\n",
    "    \"\"\"\n",
    "    Sequential Threshold Ridge Regression algorithm for finding (hopefully) sparse \n",
    "    approximation to X^{-1}y.  The idea is that this may do better with correlated observables.\n",
    "\n",
    "    This assumes y is only one column\n",
    "    \"\"\"\n",
    "\n",
    "    n,d = X0.shape\n",
    "    X = np.zeros((n,d), dtype=np.complex64)\n",
    "    # First normalize data\n",
    "    if normalize != 0:\n",
    "        Mreg = np.zeros((d,1))\n",
    "        for i in range(0,d):\n",
    "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
    "            X[:,i] = Mreg[i]*X0[:,i]\n",
    "    else: X = X0\n",
    "    \n",
    "    # Get the standard ridge esitmate\n",
    "    if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y))[0]\n",
    "    else: w = np.linalg.lstsq(X,y)[0]\n",
    "    num_relevant = d\n",
    "    biginds = np.where( abs(w) > tol)[0]\n",
    "    \n",
    "    # Threshold and continue\n",
    "    for j in range(maxit):\n",
    "\n",
    "        # Figure out which items to cut out\n",
    "        smallinds = np.where( abs(w) < tol)[0]\n",
    "        new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "            \n",
    "        # If nothing changes then stop\n",
    "        if num_relevant == len(new_biginds): break\n",
    "        else: num_relevant = len(new_biginds)\n",
    "            \n",
    "        # Also make sure we didn't just lose all the coefficients\n",
    "        if len(new_biginds) == 0:\n",
    "            if j == 0: \n",
    "                #if print_results: print \"Tolerance too high - all coefficients set below tolerance\"\n",
    "                return w\n",
    "            else: break\n",
    "        biginds = new_biginds\n",
    "        \n",
    "        # Otherwise get a new guess\n",
    "        w[smallinds] = 0\n",
    "        if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y))[0]\n",
    "        else: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
    "\n",
    "    # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
    "    \n",
    "    if normalize != 0: return np.multiply(Mreg,w)\n",
    "    else: return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe2e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10 #m轮更新\n",
    "b = 1.2 #叛逃者的收益\n",
    "K = 0.1\n",
    "P_PDG = array([[1,0],[b,0]]) #2*2的收益矩阵\n",
    "Noise = 5\n",
    "W = np.loadtxt(\"dolphin.txt\") \n",
    "T = W.shape[0] #全部社区内个体的总数量\n",
    "SV = 6\n",
    "# the part for generate EG data\n",
    "y = zeros((SV*M,T))\n",
    "AA = zeros((T,SV*M,T))\n",
    "for i in range(SV): #产生6组A和Y的值\n",
    "    [A, Aa] = data_generate(M,P_PDG,W,T,K,Noise)\n",
    "    for j in range(M):\n",
    "        y[i*M+j,:] =Aa[j,:]\n",
    "        for k in range(T):\n",
    "            AA[k,i*M+j,:] = A[k,j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0321a657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.31973238 0.52472064 1.86106773 2.07928491 3.2040047  1.69889322\n",
      "  4.99201074 0.87061827 0.23439416 1.92374683 2.28011678 2.65881852\n",
      "  0.21263218 0.55987414 4.53596223 4.02509707 0.91219193 1.31270259\n",
      "  1.86919857 4.47089115 3.10175461 3.35289012 3.5548755  0.14332207\n",
      "  2.38588407 3.1122531  2.23726241 0.26295952 2.6061696  2.05958372\n",
      "  0.54087098 0.77921968 3.59817394 3.99910005 3.04600515 2.73015595\n",
      "  3.79145282 4.79656185 0.09264504 3.40377638 4.68505742 2.96094847\n",
      "  1.65307252 1.98866877 4.28400648 3.58795702 0.84485239 3.81542351\n",
      "  4.23468022 3.70083086 2.31197907 4.62061548 2.42934279 0.4185042\n",
      "  4.47812766 3.12276248 3.54278346 0.62528617 3.25786322 0.5797584\n",
      "  0.84708126 3.53925292]\n",
      " [1.91412929 4.01473863 0.7161044  0.45099011 3.34525978 2.65716681\n",
      "  0.39252716 2.75093767 2.46460291 1.13370246 4.9964578  0.49776726\n",
      "  3.73056816 2.54938299 2.03764584 0.24872115 0.49242682 4.01615032\n",
      "  1.67879867 3.08958724 1.84913464 1.03770248 3.60877305 0.52825989\n",
      "  3.80890356 1.88593239 1.34199639 4.56148273 4.77597004 4.1663521\n",
      "  0.93472301 2.0839063  1.31597042 2.5452659  3.03280938 4.1421041\n",
      "  4.21092336 0.13664362 1.17591386 3.33052616 1.78366068 4.85191264\n",
      "  3.58364745 3.24994829 4.68641901 2.80235734 2.71707434 2.35579779\n",
      "  4.62972838 1.64384661 4.12519606 0.74182375 1.49989951 1.37308186\n",
      "  1.2070741  2.80820524 3.70736749 3.08352916 3.31139256 4.6130756\n",
      "  3.20154762 2.06073962]\n",
      " [1.68439966 3.25368587 3.67614137 3.62568688 3.81102632 2.285557\n",
      "  2.51416328 0.6058058  3.74801646 1.2495137  1.18745257 3.65636607\n",
      "  2.32170154 1.7881644  3.70941367 0.7135925  3.24459023 3.53581655\n",
      "  2.09851607 0.03123107 1.56328848 3.08669001 3.74867489 3.86019492\n",
      "  3.70059808 3.07391892 2.09754733 1.23687904 4.59256176 3.72538913\n",
      "  1.71503416 3.63483215 1.74397417 1.87132115 3.31834731 2.94926578\n",
      "  4.6142005  4.74107168 0.09668884 0.84004154 1.44543355 3.42332582\n",
      "  4.52762195 4.72902497 2.25420729 3.15957423 4.93958914 2.0518032\n",
      "  0.27160782 2.38709644 1.14428583 3.94143658 1.17079594 3.95085432\n",
      "  4.86883865 2.53724646 1.01306648 2.96676238 2.32170724 4.01063185\n",
      "  2.61665299 1.35048295]\n",
      " [0.70541975 1.07926779 1.92557745 3.52874773 1.17566871 4.57722426\n",
      "  3.01189705 0.32436571 3.97192315 2.88160758 4.79632747 2.17431154\n",
      "  0.2106794  0.19586906 2.72941431 0.11636936 1.5921129  0.99757964\n",
      "  1.83606161 4.83856496 3.20160499 1.29015943 4.55737963 3.68520684\n",
      "  4.74484353 1.02758026 2.02805236 3.4704119  3.99164376 1.36621496\n",
      "  4.86703567 2.4071501  2.05726443 0.41141039 3.69139847 0.87226558\n",
      "  2.00884838 1.14971658 1.77133547 1.04640641 1.91261306 4.84414656\n",
      "  0.32698513 4.89129636 1.23469542 1.80667068 4.50398604 4.7529681\n",
      "  0.5691219  0.02673846 1.4977452  0.72051228 3.76814784 3.20884892\n",
      "  4.87971986 3.45620152 3.40716743 0.00933961 4.46322918 2.55168932\n",
      "  1.70951777 4.12013434]\n",
      " [4.79650721 1.66970049 0.23076588 2.37053014 1.96850083 2.59577899\n",
      "  0.83343814 0.7593369  0.51652455 2.92931555 0.24944743 3.60455344\n",
      "  1.24860157 2.19656488 3.63566596 1.63946713 3.64767173 3.09449483\n",
      "  2.29038883 0.93200279 0.37398523 1.32116657 1.52224897 1.45783312\n",
      "  0.12808555 1.12096769 1.0982625  3.10839019 3.43512906 1.73868403\n",
      "  0.60903099 3.22766678 1.72633222 3.64626776 2.68175577 2.43245094\n",
      "  2.31476863 4.34835374 3.36672195 3.4597632  2.08235028 4.0054629\n",
      "  1.99105992 3.62509556 1.91298874 0.91961807 1.5167602  1.89635849\n",
      "  4.81288373 3.78324047 1.17002491 3.217163   2.28207715 4.45410846\n",
      "  2.62304923 2.7806473  4.69397887 0.37533654 2.65931453 3.07748374\n",
      "  0.56580372 1.2677689 ]\n",
      " [4.88939483 4.5243818  1.58283016 3.76994338 3.94415102 4.38199276\n",
      "  3.74383245 2.79106849 4.1515741  2.64471706 3.96151285 4.48517913\n",
      "  4.29935693 3.7500276  0.30856333 1.86157582 2.58485158 3.60259667\n",
      "  2.72045462 4.9601161  2.36926097 0.10817277 4.7521472  1.25685379\n",
      "  2.03002666 4.13337224 2.46304261 2.74078263 0.61191539 3.55083182\n",
      "  0.53062756 3.93430989 0.99369169 3.99572615 4.2871832  2.4842241\n",
      "  1.68737094 3.62112829 1.56282326 1.39196326 1.83086578 4.93318489\n",
      "  4.18112181 1.02380077 2.60354415 4.81297897 3.06410951 2.91668529\n",
      "  4.43145835 0.63984343 4.94385149 0.04004447 4.41294885 1.30794966\n",
      "  0.07983647 2.86846382 4.62389668 3.74773817 0.78296968 2.05338659\n",
      "  4.65025319 4.27731681]\n",
      " [3.85549801 1.75355872 0.95445476 1.9438268  0.90326677 2.82807982\n",
      "  0.43943528 4.66835366 2.56059796 3.98752365 3.5293624  2.67383249\n",
      "  3.46195867 1.85086298 2.82662248 2.03747598 4.04573249 2.02159143\n",
      "  3.38543886 4.72886543 2.4857385  2.81984143 3.04423153 4.62119473\n",
      "  4.08300969 4.03702886 4.99527376 3.56991633 3.99613421 2.52544844\n",
      "  4.96996438 1.11159539 1.87840969 4.68300614 2.43956268 1.45736364\n",
      "  3.35339975 1.19936286 2.5724804  3.16825166 0.02958234 1.39963498\n",
      "  4.52086009 1.70405407 0.85700498 1.84622734 3.27836781 3.26372152\n",
      "  4.34148934 4.7921737  4.70540998 4.54654328 1.75747634 4.50731798\n",
      "  0.50648427 2.0470536  1.35021961 2.22578225 3.87677307 0.45487474\n",
      "  4.04777372 1.46591313]\n",
      " [3.25144103 4.22492335 1.4798747  0.96317523 0.06434166 3.72412012\n",
      "  3.84267176 2.7127969  3.77971688 2.24754701 4.04261798 0.81386988\n",
      "  0.99008205 4.86475342 0.92319427 1.40034615 3.69625359 1.27168358\n",
      "  0.01343018 3.4722043  0.41581548 4.34946399 1.81783088 4.31611266\n",
      "  0.18989713 0.67661183 0.77618329 2.00151531 0.69782547 0.60397221\n",
      "  3.34570782 1.49819546 2.63426738 4.62783201 1.20852662 2.02542714\n",
      "  1.70269803 1.91760332 4.97475115 0.31872142 2.68715183 0.62638247\n",
      "  0.67129391 2.55948953 3.88035342 0.66699191 0.51412507 4.19396857\n",
      "  1.02359912 4.98280083 3.0755619  3.52327978 0.16207519 1.01463209\n",
      "  3.53817462 0.84613652 4.87819279 4.96114545 2.71598338 2.86874852\n",
      "  3.58512653 1.36845588]\n",
      " [2.77718347 4.3578832  3.16906067 4.8038204  0.90744649 3.87036324\n",
      "  0.87245374 4.90593318 2.367913   2.61592051 2.56771923 0.96978593\n",
      "  3.98264568 2.38839636 3.00801347 2.68215454 1.08051379 4.53374065\n",
      "  3.40344284 3.55268867 4.51569135 3.1319755  4.28853261 2.24258086\n",
      "  1.34264801 2.05384878 1.29898061 3.17396059 0.49142308 3.49583551\n",
      "  1.7485097  1.69231981 2.28721079 4.46685202 2.20202299 3.98210362\n",
      "  2.84482709 4.41107043 2.63394829 4.89879035 3.72148893 3.20155141\n",
      "  0.93722988 4.08977236 3.96422561 1.50666136 3.10923522 4.20866412\n",
      "  4.99484206 2.91739375 1.41278803 3.18504674 2.95720217 1.1340678\n",
      "  3.80272845 2.40492937 1.09184091 3.13396247 0.81752879 4.04598735\n",
      "  3.88616026 4.85469947]\n",
      " [4.69048208 1.73819758 1.347627   0.09217802 1.68953456 1.07464157\n",
      "  4.42784731 4.8893653  3.64713655 1.99569822 1.18841675 4.46727572\n",
      "  2.71141008 3.75497638 4.24516265 1.24005367 2.53264024 2.81970294\n",
      "  2.78814957 3.41716978 1.60065694 0.27861828 1.66016556 3.25620937\n",
      "  1.18034052 2.4843313  0.66608452 4.06593894 4.4355863  3.53833703\n",
      "  2.88807689 0.61227213 0.09044765 2.84844517 0.90377981 4.72700228\n",
      "  4.18698738 2.96198975 0.51276522 2.04767873 1.59315112 0.55046459\n",
      "  4.88139509 3.37315666 1.91176147 1.49660844 1.7794378  3.65249372\n",
      "  1.38809036 4.41873228 3.79728236 1.52717394 2.91548751 1.60919124\n",
      "  1.53977814 0.12588425 3.15759614 0.48693456 2.41879591 1.66232426\n",
      "  4.43998606 1.97363191]]\n"
     ]
    }
   ],
   "source": [
    "print(Noise*random.random((M,T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf2aa481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-13ac3e2d7f51>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-3-13ac3e2d7f51>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y))[0]\n",
      "<ipython-input-11-0f2aa95661a7>:5: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "for j in range(T):\n",
    "    w = TrainSTRidge(AA[j], y[:,j].reshape(-1,1),10**2,1)\n",
    "    for i in range(w.shape[0]):\n",
    "        X[i,j]=w[i,0]\n",
    "np.savetxt(\"X.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b210bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge_SPL(R, Ut, v, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "    \"\"\"\n",
    "    This function trains a predictor using STRidge_SPL.\n",
    "\n",
    "    It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "    using a loss function on a holdout set.\n",
    "\n",
    "    Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "    not squared 2-norm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "    np.random.seed(0) # for consistancy\n",
    "    n,_ = R.shape\n",
    "    train = np.random.choice(n, int(n*split), replace = False)\n",
    "    test = [i for i in np.arange(n) if i not in train]\n",
    "    TrainR = R[train,:]\n",
    "    TestR = R[test,:]\n",
    "    TrainY = Ut[train,:]\n",
    "    TestY = Ut[test,:]\n",
    "    D = TrainR.shape[1]       \n",
    "\n",
    "    # Set up the initial tolerance and l0 penalty\n",
    "    d_tol = float(d_tol)\n",
    "    tol = d_tol\n",
    "    if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
    "\n",
    "    # Get the standard least squares estimator\n",
    "    w = np.zeros((D,1))\n",
    "    w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
    "    err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "    tol_best = 0\n",
    "\n",
    "    # Now increase tolerance until test performance decreases\n",
    "    for iter in range(maxit):\n",
    "\n",
    "        # Get a set of coefficients and error\n",
    "        w = STRidge_SPL(R,Ut,v,lam,STR_iters,tol,normalize = normalize)\n",
    "        err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "\n",
    "        # Has the accuracy improved?\n",
    "        if err <= err_best:\n",
    "            err_best = err\n",
    "            w_best = w\n",
    "            tol_best = tol\n",
    "            tol = tol + d_tol\n",
    "\n",
    "        else:\n",
    "            tol = max([0,tol - 2*d_tol])\n",
    "            d_tol  = 2*d_tol / (maxit - iter)\n",
    "            tol = tol + d_tol\n",
    "\n",
    "    if print_best_tol: \n",
    "        print (\"Optimal tolerance:\", tol_best)\n",
    "    return w_best\n",
    "\n",
    "def STRidge_SPL(X0, y, v, lam, maxit, tol, normalize = 2, print_results = False):\n",
    "    \"\"\"\n",
    "    Sequential Threshold Ridge Regression algorithm for finding (hopefully) sparse \n",
    "    approximation to X^{-1}y.  The idea is that this may do better with correlated observables.\n",
    "\n",
    "    This assumes y is only one column\n",
    "    \"\"\"\n",
    "\n",
    "    n,d = X0.shape\n",
    "    X = np.zeros((n,d), dtype=np.complex64)\n",
    "    # First normalize data\n",
    "    if normalize != 0:\n",
    "        Mreg = np.zeros((d,1))\n",
    "        for i in range(0,d):\n",
    "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
    "            X[:,i] = Mreg[i]*X0[:,i]\n",
    "    else: X = X0\n",
    "    \n",
    "    # Get the standard ridge esitmate\n",
    "    if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
    "    else: w = np.linalg.lstsq(X*v,y*v)[0]\n",
    "    num_relevant = d\n",
    "    biginds = np.where( abs(w) > tol)[0]\n",
    "    \n",
    "    # Threshold and continue\n",
    "    for j in range(maxit):\n",
    "\n",
    "        # Figure out which items to cut out\n",
    "        smallinds = np.where( abs(w) < tol)[0]\n",
    "        new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "            \n",
    "        # If nothing changes then stop\n",
    "        if num_relevant == len(new_biginds): break\n",
    "        else: num_relevant = len(new_biginds)\n",
    "            \n",
    "        # Also make sure we didn't just lose all the coefficients\n",
    "        if len(new_biginds) == 0:\n",
    "            if j == 0: \n",
    "                #if print_results: print \"Tolerance too high - all coefficients set below tolerance\"\n",
    "                return w\n",
    "            else: break\n",
    "        biginds = new_biginds\n",
    "        \n",
    "        # Otherwise get a new guess\n",
    "        w[smallinds] = 0\n",
    "        if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n",
    "        else: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
    "\n",
    "    # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
    "    \n",
    "    if normalize != 0: return np.multiply(Mreg,w)\n",
    "    else: return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c4d6f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-5e04d6e95ebc>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-12-5e04d6e95ebc>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
      "<ipython-input-72-d312225d135e>:18: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n",
      "<ipython-input-12-5e04d6e95ebc>:106: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
      "<ipython-input-12-5e04d6e95ebc>:106: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
      "<ipython-input-12-5e04d6e95ebc>:102: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "v = ones((SV*M,1))\n",
    "v1 = zeros((SV*M,1))\n",
    "Los = zeros((SV*M,T))\n",
    "losss = zeros((SV*M,1))\n",
    "lambda_0=2\n",
    "t=0\n",
    "while (t<30):  \n",
    "    if (v==v1).all():\n",
    "        break\n",
    "    else:\n",
    "        for j in range(T):\n",
    "            w = TrainSTRidge_SPL(AA[j], y[:,j].reshape(-1,1), v,10**2,1)\n",
    "            Lo =  abs((np.dot(AA[j], w))-y[:,j].reshape(-1,1))\n",
    "            for i in range(SV*M):\n",
    "                Los[i,j]=Lo[i]\n",
    "            for i in range(w.shape[0]):\n",
    "                X[i,j]=w[i,0]\n",
    "        for i in range(SV*M):\n",
    "            losss[i]=np.mean(Los[i,:])\n",
    "        t = t+1\n",
    "        for i in range(SV*M):\n",
    "            if losss[i]<lambda_0:\n",
    "                v[i]=1-losss[i]/lambda_0\n",
    "            else:\n",
    "                v[i]=0\n",
    "        lambda_0 = lambda_0*1.2\n",
    "        #lambda_0 = lambda_0 + np.mean(losss)      \n",
    "np.savetxt(\"X1.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff62e5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9955816 ],\n",
       "       [0.99618877],\n",
       "       [0.99570593],\n",
       "       [0.99559106],\n",
       "       [0.99661468],\n",
       "       [0.99606064],\n",
       "       [0.996653  ],\n",
       "       [0.99544602],\n",
       "       [0.99535572],\n",
       "       [0.99445454],\n",
       "       [0.99654712],\n",
       "       [0.99551444],\n",
       "       [0.99660482],\n",
       "       [0.9967245 ],\n",
       "       [0.99655672],\n",
       "       [0.99625789],\n",
       "       [0.99557681],\n",
       "       [0.99657966],\n",
       "       [0.99604689],\n",
       "       [0.99566116],\n",
       "       [0.99627342],\n",
       "       [0.99664808],\n",
       "       [0.99669432],\n",
       "       [0.99569287],\n",
       "       [0.99637004],\n",
       "       [0.99577093],\n",
       "       [0.9962897 ],\n",
       "       [0.9957412 ],\n",
       "       [0.99544689],\n",
       "       [0.99555285],\n",
       "       [0.99638488],\n",
       "       [0.99647795],\n",
       "       [0.99521225],\n",
       "       [0.99524348],\n",
       "       [0.99498347],\n",
       "       [0.99472013],\n",
       "       [0.99505912],\n",
       "       [0.99492707],\n",
       "       [0.99516052],\n",
       "       [0.99552895],\n",
       "       [0.99587629],\n",
       "       [0.99611192],\n",
       "       [0.99567016],\n",
       "       [0.99655976],\n",
       "       [0.99594253],\n",
       "       [0.99627597],\n",
       "       [0.99604146],\n",
       "       [0.99639975],\n",
       "       [0.99573486],\n",
       "       [0.9961013 ],\n",
       "       [0.99697267],\n",
       "       [0.99652488],\n",
       "       [0.99633006],\n",
       "       [0.99577258],\n",
       "       [0.99566409],\n",
       "       [0.99602547],\n",
       "       [0.99595061],\n",
       "       [0.99623569],\n",
       "       [0.99620521],\n",
       "       [0.99664261]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e00da151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-5e04d6e95ebc>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-12-5e04d6e95ebc>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
      "<ipython-input-74-9b9da0bd393e>:17: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n",
      "<ipython-input-12-5e04d6e95ebc>:102: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n",
      "<ipython-input-12-5e04d6e95ebc>:106: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
      "<ipython-input-12-5e04d6e95ebc>:106: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "v = ones((SV*M,T))\n",
    "v1 = zeros((SV*M,T))\n",
    "Los = zeros((SV*M,T))\n",
    "lambda_0=2\n",
    "t=0\n",
    "while (t<25):\n",
    "    if (v==v1).all():\n",
    "        break\n",
    "    else:\n",
    "        for j in range(T):\n",
    "            w = TrainSTRidge_SPL(AA[j], y[:,j].reshape(-1,1), v[:,j].reshape(-1,1),10**2,1)\n",
    "            Lo =  abs((np.dot(AA[j], w))-y[:,j].reshape(-1,1))\n",
    "            for i in range(SV*M):\n",
    "                Los[i,j]=Lo[i]\n",
    "            for i in range(w.shape[0]):\n",
    "                X[i,j]=w[i,0]\n",
    "        t = t+1\n",
    "        for j in range(T):\n",
    "            for i in range(SV*M):\n",
    "                if Los[i,j]<lambda_0:\n",
    "                    v[i,j]=1-Los[i,j]/lambda_0\n",
    "                else:\n",
    "                    v[i,j]=0\n",
    "        lambda_0 = lambda_0*1.25\n",
    "        #lambda_0 = lambda_0 + np.mean(Los)\n",
    "np.savetxt(\"X2.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "15d2e7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99360852, 0.99182599, 0.99306591, ..., 0.99611834, 0.9996592 ,\n",
       "        0.99598325],\n",
       "       [0.99269985, 0.98641488, 0.99110899, ..., 0.99333621, 0.99759457,\n",
       "        0.99468098],\n",
       "       [0.99941523, 0.98453523, 0.99335071, ..., 0.99822469, 0.99818597,\n",
       "        0.99998158],\n",
       "       ...,\n",
       "       [0.99440547, 0.9996112 , 0.99340701, ..., 0.99369005, 0.99806001,\n",
       "        0.99836005],\n",
       "       [0.98805081, 0.99906964, 0.99990961, ..., 0.9889458 , 0.9988205 ,\n",
       "        0.99472598],\n",
       "       [0.99679979, 0.99669256, 0.99440044, ..., 0.9926274 , 0.99999685,\n",
       "        0.99339232]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86360c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
