{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2990296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "from random import shuffle\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9b6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generate(M,P_PDG,W,N,K,Noise):\n",
    "    #W是原数据集,代表哪些代理之间有连接，即网络\n",
    "    #N是W的行数，即代理的个数\n",
    "    #代理人的博弈策略\n",
    "    player = zeros((M+1,N))\n",
    "    #原始博弈策略放在第一行，随机设置\n",
    "    for i in range(N):\n",
    "        if random.random()<=0.5:\n",
    "            player[0,i] = 1 #1代表合作\n",
    "        else:\n",
    "            player[0,i] = 0 #0代表不合作\n",
    "    #计算每个节点的收益\n",
    "    #M轮博弈\n",
    "    F = zeros((1,N))\n",
    "    G = zeros((M,N))\n",
    "    A = zeros((N,M,N))\n",
    "    for t in range(M):\n",
    "        for i in range(N):\n",
    "            if player[t,i] == 1:\n",
    "                s1 = array([[1],[0]])\n",
    "            else:\n",
    "                s1 = array([[0],[1]])\n",
    "            for j in range(N):\n",
    "                if player[t,j] == 0:\n",
    "                    s2 = array([[0],[1]])\n",
    "                else:\n",
    "                    s2 = array([[1],[0]])\n",
    "                F[0,j] = ((s1.T).dot(P_PDG)).dot(s2) #如果代理i和j连接，则代理i的收益为F[0,j]\n",
    "            A[i,t,:] = F  #A:N*M*N   \n",
    "            # F是三维矩阵A的第i页，第t行\n",
    "            G[t,i] = F.dot(W[:,i]) #第t轮代理i的收益\n",
    "        # update strategies\n",
    "        for k in range(N):\n",
    "            s=[i for i,x in enumerate(list(W[:,k])) if x>=1]     # 找出与代理k合作的代理的索引\n",
    "            if len(s)!=0: # 如果有代理与代理k合作\n",
    "                shuffle(s)\n",
    "                P = 1/(1+math.e**((G[t,k]-G[t,s[0]])/K)) # 费米规则\n",
    "                if random.random()<= P:\n",
    "                    player[t+1,k] = player[t,s[0]]\n",
    "                else:\n",
    "                    player[t+1,k] = player[t,k]\n",
    "            else: # 如果没有代理与代理k合作\n",
    "                player[t+1,k] = player[t,k]\n",
    "    # add noise for G\n",
    "    Aa = G + Noise*random.random((M,N)) # m轮收益总矩阵：包含每一轮的收益M*N\n",
    "    return [A,Aa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40594aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge(R, Ut, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "    \"\"\"\n",
    "    This function trains a predictor using STRidge.\n",
    "\n",
    "    It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "    using a loss function on a holdout set.\n",
    "\n",
    "    Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "    not squared 2-norm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "    np.random.seed(0) # for consistancy\n",
    "    n,_ = R.shape\n",
    "    train = np.random.choice(n, int(n*split), replace = False)\n",
    "    test = [i for i in np.arange(n) if i not in train]\n",
    "    TrainR = R[train,:]\n",
    "    TestR = R[test,:]\n",
    "    TrainY = Ut[train,:]\n",
    "    TestY = Ut[test,:]\n",
    "    D = TrainR.shape[1]       \n",
    "\n",
    "    # Set up the initial tolerance and l0 penalty\n",
    "    d_tol = float(d_tol)\n",
    "    tol = d_tol\n",
    "    if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
    "\n",
    "    # Get the standard least squares estimator\n",
    "    w = np.zeros((D,1))\n",
    "    w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
    "    err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "    tol_best = 0\n",
    "\n",
    "    # Now increase tolerance until test performance decreases\n",
    "    for iter in range(maxit):\n",
    "\n",
    "        # Get a set of coefficients and error\n",
    "        w = STRidge(R,Ut,lam,STR_iters,tol,normalize = normalize)\n",
    "        err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "\n",
    "        # Has the accuracy improved?\n",
    "        if err <= err_best:\n",
    "            err_best = err\n",
    "            w_best = w\n",
    "            tol_best = tol\n",
    "            tol = tol + d_tol\n",
    "\n",
    "        else:\n",
    "            tol = max([0,tol - 2*d_tol])\n",
    "            d_tol  = 2*d_tol / (maxit - iter)\n",
    "            tol = tol + d_tol\n",
    "\n",
    "    if print_best_tol: \n",
    "        print (\"Optimal tolerance:\", tol_best)\n",
    "    return w_best\n",
    "\n",
    "def STRidge(X0, y, lam, maxit, tol, normalize = 2, print_results = False):\n",
    "    \"\"\"\n",
    "    Sequential Threshold Ridge Regression algorithm for finding (hopefully) sparse \n",
    "    approximation to X^{-1}y.  The idea is that this may do better with correlated observables.\n",
    "\n",
    "    This assumes y is only one column\n",
    "    \"\"\"\n",
    "\n",
    "    n,d = X0.shape\n",
    "    X = np.zeros((n,d), dtype=np.complex64)\n",
    "    # First normalize data\n",
    "    if normalize != 0:\n",
    "        Mreg = np.zeros((d,1))\n",
    "        for i in range(0,d):\n",
    "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
    "            X[:,i] = Mreg[i]*X0[:,i]\n",
    "    else: X = X0\n",
    "    \n",
    "    # Get the standard ridge esitmate\n",
    "    if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y))[0]\n",
    "    else: w = np.linalg.lstsq(X,y)[0]\n",
    "    num_relevant = d\n",
    "    biginds = np.where( abs(w) > tol)[0]\n",
    "    \n",
    "    # Threshold and continue\n",
    "    for j in range(maxit):\n",
    "\n",
    "        # Figure out which items to cut out\n",
    "        smallinds = np.where( abs(w) < tol)[0]\n",
    "        new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "            \n",
    "        # If nothing changes then stop\n",
    "        if num_relevant == len(new_biginds): break\n",
    "        else: num_relevant = len(new_biginds)\n",
    "            \n",
    "        # Also make sure we didn't just lose all the coefficients\n",
    "        if len(new_biginds) == 0:\n",
    "            if j == 0: \n",
    "                #if print_results: print \"Tolerance too high - all coefficients set below tolerance\"\n",
    "                return w\n",
    "            else: break\n",
    "        biginds = new_biginds\n",
    "        \n",
    "        # Otherwise get a new guess\n",
    "        w[smallinds] = 0\n",
    "        if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y))[0]\n",
    "        else: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
    "\n",
    "    # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
    "    \n",
    "    if normalize != 0: return np.multiply(Mreg,w)\n",
    "    else: return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe2e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10 #m轮更新\n",
    "b = 1.2 #叛逃者的收益\n",
    "K = 0.1\n",
    "P_PDG = array([[1,0],[b,0]]) #2*2的收益矩阵\n",
    "Noise = 5\n",
    "W = np.loadtxt(\"BA.txt\") \n",
    "T = W.shape[0] #全部社区内个体的总数量\n",
    "SV = 6\n",
    "# the part for generate EG data\n",
    "y = zeros((SV*M,T))\n",
    "AA = zeros((T,SV*M,T))\n",
    "for i in range(SV): #产生6组A和Y的值\n",
    "    [A, Aa] = data_generate(M,P_PDG,W,T,K,Noise)\n",
    "    for j in range(M):\n",
    "        y[i*M+j,:] =Aa[j,:]\n",
    "        for k in range(T):\n",
    "            AA[k,i*M+j,:] = A[k,j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe860929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.33845485 3.36189134 2.10392411 1.40443759 0.49083518 0.52583495\n",
      "  0.56784927 3.19346394 4.58449113 1.18602334 4.05936244 2.05324132\n",
      "  1.46042735 0.76339307 3.81135628 1.26698443 1.97194928 2.35451691\n",
      "  3.55674709 2.61008694 2.73560337 1.95760229 0.95104373 0.55914967\n",
      "  3.09981463 4.73360205 0.07119996 0.57039477 0.43531548 3.67247532\n",
      "  4.79370159 2.9727993  2.6800013  4.59873391 4.86367861 0.92342386\n",
      "  2.90816554 0.2089273  3.42491307 1.13683283]\n",
      " [2.53483879 3.06248326 3.07430112 0.00879446 0.30816167 1.2056525\n",
      "  1.24620025 0.20221674 0.66000523 0.66156344 1.87661507 4.47890572\n",
      "  3.03618868 4.68262707 4.00295792 4.67839504 2.74793662 2.16653256\n",
      "  0.22796125 2.94958728 3.11532857 3.85192962 4.13720909 3.36824033\n",
      "  2.15563941 1.27401768 2.30145466 4.13351017 2.85726494 3.25200319\n",
      "  1.1855454  3.97855322 2.69138172 1.73726176 4.2941575  0.77680128\n",
      "  2.95886102 0.05763388 4.07088628 2.90945188]\n",
      " [2.5271408  3.06731424 4.75871306 3.15221433 3.91097349 3.95162677\n",
      "  2.01886022 3.92691295 4.69845973 0.12325361 2.4614366  1.29870564\n",
      "  4.30263859 4.42671773 1.90773025 4.9915886  4.91280949 0.93530323\n",
      "  4.36702707 2.08840548 4.30346265 0.63587941 2.84997158 2.57480318\n",
      "  4.31664358 2.08882793 0.40900632 1.68874625 4.3104504  2.28379253\n",
      "  1.75247159 0.57998899 1.90802595 3.53230952 3.55039463 0.06191016\n",
      "  0.46228753 3.98066088 1.89334472 1.81153946]\n",
      " [2.46795845 0.44981865 0.90350255 4.80296892 2.89972885 4.25698775\n",
      "  1.08639543 3.60157829 2.42666561 1.9364217  2.38181383 0.24744597\n",
      "  0.07134216 0.10352093 1.40663353 1.1978046  2.67404851 0.61819838\n",
      "  4.79045833 4.39210823 1.07938921 3.65634264 2.26974161 0.74412546\n",
      "  4.12547042 2.01210014 3.8398391  4.09073396 3.89529681 3.27100702\n",
      "  3.07420092 2.46313627 4.75177384 0.08974084 2.26190826 4.20589883\n",
      "  2.01182194 2.55834835 3.58527404 2.69630771]\n",
      " [4.67283191 1.36461298 2.11050477 2.63794302 0.97020578 0.35444746\n",
      "  4.28508098 3.22556367 2.72467105 4.03605887 3.02364601 3.66330411\n",
      "  4.50660917 3.96999382 2.07909677 3.79272562 0.41333048 3.61562847\n",
      "  4.51666619 3.04613655 2.10024007 4.57465165 3.89046639 4.05003671\n",
      "  4.25550144 3.21575767 2.75383857 3.34187078 1.83973802 0.59709549\n",
      "  3.08412232 1.88358784 4.09949448 2.29980322 1.12145221 1.64285082\n",
      "  0.8813355  4.93415007 0.31222826 4.32137408]\n",
      " [0.03793067 2.41248229 4.00764986 3.05426454 2.6981453  1.78467678\n",
      "  3.05689666 3.63553716 0.77324452 2.17028674 4.93605762 3.32227858\n",
      "  0.91579093 1.0151062  1.39865971 3.51366039 3.00924836 2.63107382\n",
      "  1.18777773 2.58807814 1.69438763 3.85664926 0.7133101  4.21509427\n",
      "  2.29082359 1.39834522 2.86866024 1.89019429 2.19832857 2.47136134\n",
      "  3.62791592 3.26680561 3.66979339 3.79026265 3.11139135 2.02262468\n",
      "  4.48073318 4.58581134 2.25095067 4.80971715]\n",
      " [0.35105774 4.21243481 4.38310437 0.69905253 1.09305618 0.57463842\n",
      "  0.83565316 1.77668999 4.93566539 0.23468819 4.57534566 0.54614209\n",
      "  1.34127523 2.05073518 3.57999133 4.54861549 2.52641345 3.74924978\n",
      "  1.39143611 3.17604872 2.64190169 0.72034942 3.97204673 3.00250355\n",
      "  2.63967464 0.5271515  0.17088363 2.80029084 4.438853   3.73825466\n",
      "  4.07315872 1.91650834 3.34760539 3.59385771 4.48910427 0.55391462\n",
      "  1.0480833  1.70486452 0.72341675 3.70398708]\n",
      " [3.01862775 2.84833405 3.71312791 3.37501871 0.4450032  1.63958307\n",
      "  3.40795639 0.75279409 2.63591281 4.95287765 4.11754072 0.08808901\n",
      "  2.94795956 0.14674271 4.53259371 0.11729071 3.5587427  0.26521765\n",
      "  0.53462752 2.58351197 3.61427627 1.05811974 2.02317986 1.12626128\n",
      "  2.70959421 0.16148298 2.60636456 2.99922302 1.61731706 1.02817622\n",
      "  0.86350487 0.16551011 4.51392691 1.12636172 4.96491749 0.59847081\n",
      "  4.58942988 4.64902214 1.67111603 1.2842684 ]\n",
      " [0.18013492 0.87359425 1.54501542 2.44063661 1.3574911  3.21353671\n",
      "  0.79685935 4.25307982 3.68036276 1.13651757 4.23038744 0.72389561\n",
      "  0.94914236 2.11634375 3.43096019 4.97229654 0.66413333 2.32733769\n",
      "  1.30208851 2.88409367 4.05601675 1.35204881 3.30886704 4.03179256\n",
      "  4.33352244 3.17428468 3.56409743 2.14125535 4.50204023 0.67832361\n",
      "  4.40794405 3.78561385 1.41633611 0.4472525  2.67416083 0.32676975\n",
      "  0.31001278 3.61797093 2.75895665 4.72306576]\n",
      " [0.75709082 3.4439187  2.40592617 4.8337517  3.15162526 0.55019398\n",
      "  3.52824119 3.12183979 3.5707799  2.73569179 1.17289985 4.29793514\n",
      "  1.88145859 2.99407938 4.68386567 0.07192265 1.91088298 4.15281404\n",
      "  4.04692717 0.27802115 3.72434727 4.73690227 1.26555212 4.80878352\n",
      "  1.98396626 2.94876466 4.89912894 3.26999414 2.57172232 3.28768739\n",
      "  2.09824957 2.51735675 0.53993965 2.55877201 3.8043765  3.83074468\n",
      "  0.26007243 2.25976998 3.93237378 4.10542741]]\n"
     ]
    }
   ],
   "source": [
    "print(Noise*random.random((M,T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf2aa481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-13ac3e2d7f51>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-3-13ac3e2d7f51>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y))[0]\n",
      "<ipython-input-3-13ac3e2d7f51>:102: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y))[0]\n",
      "<ipython-input-3-13ac3e2d7f51>:106: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
      "<ipython-input-9-3c4d03b29948>:5: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n",
      "<ipython-input-3-13ac3e2d7f51>:106: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "for j in range(T):\n",
    "    w = TrainSTRidge(AA[j], y[:,j].reshape(-1,1),10**0,1)\n",
    "    for i in range(w.shape[0]):\n",
    "        X[i,j]=w[i,0]\n",
    "np.savetxt(\"X.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b210bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge_SPL(R, Ut, v, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "    \"\"\"\n",
    "    This function trains a predictor using STRidge_SPL.\n",
    "\n",
    "    It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "    using a loss function on a holdout set.\n",
    "\n",
    "    Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "    not squared 2-norm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "    np.random.seed(0) # for consistancy\n",
    "    n,_ = R.shape\n",
    "    train = np.random.choice(n, int(n*split), replace = False)\n",
    "    test = [i for i in np.arange(n) if i not in train]\n",
    "    TrainR = R[train,:]\n",
    "    TestR = R[test,:]\n",
    "    TrainY = Ut[train,:]\n",
    "    TestY = Ut[test,:]\n",
    "    D = TrainR.shape[1]       \n",
    "\n",
    "    # Set up the initial tolerance and l0 penalty\n",
    "    d_tol = float(d_tol)\n",
    "    tol = d_tol\n",
    "    if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
    "\n",
    "    # Get the standard least squares estimator\n",
    "    w = np.zeros((D,1))\n",
    "    w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
    "    err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "    tol_best = 0\n",
    "\n",
    "    # Now increase tolerance until test performance decreases\n",
    "    for iter in range(maxit):\n",
    "\n",
    "        # Get a set of coefficients and error\n",
    "        w = STRidge_SPL(R,Ut,v,lam,STR_iters,tol,normalize = normalize)\n",
    "        err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "\n",
    "        # Has the accuracy improved?\n",
    "        if err <= err_best:\n",
    "            err_best = err\n",
    "            w_best = w\n",
    "            tol_best = tol\n",
    "            tol = tol + d_tol\n",
    "\n",
    "        else:\n",
    "            tol = max([0,tol - 2*d_tol])\n",
    "            d_tol  = 2*d_tol / (maxit - iter)\n",
    "            tol = tol + d_tol\n",
    "\n",
    "    if print_best_tol: \n",
    "        print (\"Optimal tolerance:\", tol_best)\n",
    "    return w_best\n",
    "\n",
    "def STRidge_SPL(X0, y, v, lam, maxit, tol, normalize = 2, print_results = False):\n",
    "    \"\"\"\n",
    "    Sequential Threshold Ridge Regression algorithm for finding (hopefully) sparse \n",
    "    approximation to X^{-1}y.  The idea is that this may do better with correlated observables.\n",
    "\n",
    "    This assumes y is only one column\n",
    "    \"\"\"\n",
    "\n",
    "    n,d = X0.shape\n",
    "    X = np.zeros((n,d), dtype=np.complex64)\n",
    "    # First normalize data\n",
    "    if normalize != 0:\n",
    "        Mreg = np.zeros((d,1))\n",
    "        for i in range(0,d):\n",
    "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
    "            X[:,i] = Mreg[i]*X0[:,i]\n",
    "    else: X = X0\n",
    "    \n",
    "    # Get the standard ridge esitmate\n",
    "    if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
    "    else: w = np.linalg.lstsq(X*v,y*v)[0]\n",
    "    num_relevant = d\n",
    "    biginds = np.where( abs(w) > tol)[0]\n",
    "    \n",
    "    # Threshold and continue\n",
    "    for j in range(maxit):\n",
    "\n",
    "        # Figure out which items to cut out\n",
    "        smallinds = np.where( abs(w) < tol)[0]\n",
    "        new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "            \n",
    "        # If nothing changes then stop\n",
    "        if num_relevant == len(new_biginds): break\n",
    "        else: num_relevant = len(new_biginds)\n",
    "            \n",
    "        # Also make sure we didn't just lose all the coefficients\n",
    "        if len(new_biginds) == 0:\n",
    "            if j == 0: \n",
    "                #if print_results: print \"Tolerance too high - all coefficients set below tolerance\"\n",
    "                return w\n",
    "            else: break\n",
    "        biginds = new_biginds\n",
    "        \n",
    "        # Otherwise get a new guess\n",
    "        w[smallinds] = 0\n",
    "        if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n",
    "        else: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
    "\n",
    "    # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
    "    \n",
    "    if normalize != 0: return np.multiply(Mreg,w)\n",
    "    else: return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4d6f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-5e04d6e95ebc>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-10-5e04d6e95ebc>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
      "<ipython-input-10-5e04d6e95ebc>:102: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n",
      "<ipython-input-10-5e04d6e95ebc>:106: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
      "<ipython-input-14-06742e5fa348>:18: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n",
      "<ipython-input-10-5e04d6e95ebc>:106: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "v = ones((SV*M,1))\n",
    "v1 = zeros((SV*M,1))\n",
    "Los = zeros((SV*M,T))\n",
    "losss = zeros((SV*M,1))\n",
    "lambda_0=1\n",
    "t=0\n",
    "while (t<35):  \n",
    "    if (v==v1).all():\n",
    "        break\n",
    "    else:\n",
    "        for j in range(T):\n",
    "            w = TrainSTRidge_SPL(AA[j], y[:,j].reshape(-1,1), v,10**0,1)\n",
    "            Lo =  abs((np.dot(AA[j], w))-y[:,j].reshape(-1,1))\n",
    "            for i in range(SV*M):\n",
    "                Los[i,j]=Lo[i]\n",
    "            for i in range(w.shape[0]):\n",
    "                X[i,j]=w[i,0]\n",
    "        for i in range(SV*M):\n",
    "            losss[i]=np.mean(Los[i,:])\n",
    "        t = t+1\n",
    "        for i in range(SV*M):\n",
    "            if losss[i]<lambda_0:\n",
    "                v[i]=1-losss[i]/lambda_0\n",
    "            else:\n",
    "                v[i]=0\n",
    "        lambda_0 = lambda_0*1.25\n",
    "        #lambda_0 = lambda_0 + np.mean(losss)      \n",
    "np.savetxt(\"X1.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff62e5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99964884],\n",
       "       [0.9994012 ],\n",
       "       [0.99950739],\n",
       "       [0.99958157],\n",
       "       [0.99947525],\n",
       "       [0.99949577],\n",
       "       [0.99946908],\n",
       "       [0.9994305 ],\n",
       "       [0.99942144],\n",
       "       [0.99947414],\n",
       "       [0.99948745],\n",
       "       [0.99952782],\n",
       "       [0.99948094],\n",
       "       [0.99950939],\n",
       "       [0.99945706],\n",
       "       [0.99953031],\n",
       "       [0.99953535],\n",
       "       [0.99950843],\n",
       "       [0.99945709],\n",
       "       [0.99954756],\n",
       "       [0.99959869],\n",
       "       [0.9995633 ],\n",
       "       [0.99956463],\n",
       "       [0.99952797],\n",
       "       [0.99943882],\n",
       "       [0.99944487],\n",
       "       [0.99958067],\n",
       "       [0.99944768],\n",
       "       [0.99943544],\n",
       "       [0.99937078],\n",
       "       [0.99957683],\n",
       "       [0.99964048],\n",
       "       [0.9995251 ],\n",
       "       [0.99946462],\n",
       "       [0.99952376],\n",
       "       [0.99956155],\n",
       "       [0.99947113],\n",
       "       [0.99939002],\n",
       "       [0.99957766],\n",
       "       [0.99955122],\n",
       "       [0.99965389],\n",
       "       [0.99972118],\n",
       "       [0.99951731],\n",
       "       [0.99951068],\n",
       "       [0.99954081],\n",
       "       [0.99949439],\n",
       "       [0.99949896],\n",
       "       [0.99964513],\n",
       "       [0.99956295],\n",
       "       [0.99948773],\n",
       "       [0.99964869],\n",
       "       [0.99959956],\n",
       "       [0.99953183],\n",
       "       [0.99949452],\n",
       "       [0.99954321],\n",
       "       [0.99958342],\n",
       "       [0.99946002],\n",
       "       [0.99948079],\n",
       "       [0.99942283],\n",
       "       [0.99951393]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e00da151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-5e04d6e95ebc>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-10-5e04d6e95ebc>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
      "<ipython-input-10-5e04d6e95ebc>:102: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n",
      "<ipython-input-10-5e04d6e95ebc>:106: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
      "<ipython-input-20-7f66155017c4>:17: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n",
      "<ipython-input-10-5e04d6e95ebc>:106: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "v = ones((SV*M,T))\n",
    "v1 = zeros((SV*M,T))\n",
    "Los = zeros((SV*M,T))\n",
    "lambda_0=1\n",
    "t=0\n",
    "while (t<40):\n",
    "    if (v==v1).all():\n",
    "        break\n",
    "    else:\n",
    "        for j in range(T):\n",
    "            w = TrainSTRidge_SPL(AA[j], y[:,j].reshape(-1,1), v[:,j].reshape(-1,1),10**0,1)\n",
    "            Lo =  abs((np.dot(AA[j], w))-y[:,j].reshape(-1,1))\n",
    "            for i in range(SV*M):\n",
    "                Los[i,j]=Lo[i]\n",
    "            for i in range(w.shape[0]):\n",
    "                X[i,j]=w[i,0]\n",
    "        t = t+1\n",
    "        for j in range(T):\n",
    "            for i in range(SV*M):\n",
    "                if Los[i,j]<lambda_0:\n",
    "                    v[i,j]=1-Los[i,j]/lambda_0\n",
    "                else:\n",
    "                    v[i,j]=0\n",
    "        lambda_0 = lambda_0*1.25\n",
    "        #lambda_0 = lambda_0 + np.mean(Los)\n",
    "np.savetxt(\"X2.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15d2e7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99993417, 0.9998871 , 0.99978017, ..., 0.99950231, 0.99999995,\n",
       "        0.99978962],\n",
       "       [0.99935015, 0.9990428 , 0.99972436, ..., 0.99934073, 0.99812392,\n",
       "        0.99927207],\n",
       "       [0.99977427, 0.99926651, 0.99961033, ..., 0.99979732, 0.99957917,\n",
       "        0.99961069],\n",
       "       ...,\n",
       "       [0.99962752, 0.9996243 , 0.99973385, ..., 0.99854164, 0.99925716,\n",
       "        0.999532  ],\n",
       "       [0.99977251, 0.99931357, 0.99982283, ..., 0.99977256, 0.99902785,\n",
       "        0.99888759],\n",
       "       [0.99933667, 0.99938814, 0.99964254, ..., 0.99912681, 0.99927781,\n",
       "        0.99970717]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86360c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
