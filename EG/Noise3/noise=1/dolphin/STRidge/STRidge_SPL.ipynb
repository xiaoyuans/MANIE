{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2990296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "from random import shuffle\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9b6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generate(M,P_PDG,W,N,K,Noise):\n",
    "    #W是原数据集,代表哪些代理之间有连接，即网络\n",
    "    #N是W的行数，即代理的个数\n",
    "    #代理人的博弈策略\n",
    "    player = zeros((M+1,N))\n",
    "    #原始博弈策略放在第一行，随机设置\n",
    "    for i in range(N):\n",
    "        if random.random()<=0.5:\n",
    "            player[0,i] = 1 #1代表合作\n",
    "        else:\n",
    "            player[0,i] = 0 #0代表不合作\n",
    "    #计算每个节点的收益\n",
    "    #M轮博弈\n",
    "    F = zeros((1,N))\n",
    "    G = zeros((M,N))\n",
    "    A = zeros((N,M,N))\n",
    "    for t in range(M):\n",
    "        for i in range(N):\n",
    "            if player[t,i] == 1:\n",
    "                s1 = array([[1],[0]])\n",
    "            else:\n",
    "                s1 = array([[0],[1]])\n",
    "            for j in range(N):\n",
    "                if player[t,j] == 0:\n",
    "                    s2 = array([[0],[1]])\n",
    "                else:\n",
    "                    s2 = array([[1],[0]])\n",
    "                F[0,j] = ((s1.T).dot(P_PDG)).dot(s2) #如果代理i和j连接，则代理i的收益为F[0,j]\n",
    "            A[i,t,:] = F  #A:N*M*N   \n",
    "            # F是三维矩阵A的第i页，第t行\n",
    "            G[t,i] = F.dot(W[:,i]) #第t轮代理i的收益\n",
    "        # update strategies\n",
    "        for k in range(N):\n",
    "            s=[i for i,x in enumerate(list(W[:,k])) if x>=1]     # 找出与代理k合作的代理的索引\n",
    "            if len(s)!=0: # 如果有代理与代理k合作\n",
    "                shuffle(s)\n",
    "                P = 1/(1+math.e**((G[t,k]-G[t,s[0]])/K)) # 费米规则\n",
    "                if random.random()<= P:\n",
    "                    player[t+1,k] = player[t,s[0]]\n",
    "                else:\n",
    "                    player[t+1,k] = player[t,k]\n",
    "            else: # 如果没有代理与代理k合作\n",
    "                player[t+1,k] = player[t,k]\n",
    "    # add noise for G\n",
    "    Aa = G + Noise*random.random((M,N)) # m轮收益总矩阵：包含每一轮的收益M*N\n",
    "    return [A,Aa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40594aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge(R, Ut, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "    \"\"\"\n",
    "    This function trains a predictor using STRidge.\n",
    "\n",
    "    It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "    using a loss function on a holdout set.\n",
    "\n",
    "    Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "    not squared 2-norm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "    np.random.seed(0) # for consistancy\n",
    "    n,_ = R.shape\n",
    "    train = np.random.choice(n, int(n*split), replace = False)\n",
    "    test = [i for i in np.arange(n) if i not in train]\n",
    "    TrainR = R[train,:]\n",
    "    TestR = R[test,:]\n",
    "    TrainY = Ut[train,:]\n",
    "    TestY = Ut[test,:]\n",
    "    D = TrainR.shape[1]       \n",
    "\n",
    "    # Set up the initial tolerance and l0 penalty\n",
    "    d_tol = float(d_tol)\n",
    "    tol = d_tol\n",
    "    if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
    "\n",
    "    # Get the standard least squares estimator\n",
    "    w = np.zeros((D,1))\n",
    "    w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
    "    err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "    tol_best = 0\n",
    "\n",
    "    # Now increase tolerance until test performance decreases\n",
    "    for iter in range(maxit):\n",
    "\n",
    "        # Get a set of coefficients and error\n",
    "        w = STRidge(R,Ut,lam,STR_iters,tol,normalize = normalize)\n",
    "        err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "\n",
    "        # Has the accuracy improved?\n",
    "        if err <= err_best:\n",
    "            err_best = err\n",
    "            w_best = w\n",
    "            tol_best = tol\n",
    "            tol = tol + d_tol\n",
    "\n",
    "        else:\n",
    "            tol = max([0,tol - 2*d_tol])\n",
    "            d_tol  = 2*d_tol / (maxit - iter)\n",
    "            tol = tol + d_tol\n",
    "\n",
    "    if print_best_tol: \n",
    "        print (\"Optimal tolerance:\", tol_best)\n",
    "    return w_best\n",
    "\n",
    "def STRidge(X0, y, lam, maxit, tol, normalize = 2, print_results = False):\n",
    "    \"\"\"\n",
    "    Sequential Threshold Ridge Regression algorithm for finding (hopefully) sparse \n",
    "    approximation to X^{-1}y.  The idea is that this may do better with correlated observables.\n",
    "\n",
    "    This assumes y is only one column\n",
    "    \"\"\"\n",
    "\n",
    "    n,d = X0.shape\n",
    "    X = np.zeros((n,d), dtype=np.complex64)\n",
    "    # First normalize data\n",
    "    if normalize != 0:\n",
    "        Mreg = np.zeros((d,1))\n",
    "        for i in range(0,d):\n",
    "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
    "            X[:,i] = Mreg[i]*X0[:,i]\n",
    "    else: X = X0\n",
    "    \n",
    "    # Get the standard ridge esitmate\n",
    "    if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y))[0]\n",
    "    else: w = np.linalg.lstsq(X,y)[0]\n",
    "    num_relevant = d\n",
    "    biginds = np.where( abs(w) > tol)[0]\n",
    "    \n",
    "    # Threshold and continue\n",
    "    for j in range(maxit):\n",
    "\n",
    "        # Figure out which items to cut out\n",
    "        smallinds = np.where( abs(w) < tol)[0]\n",
    "        new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "            \n",
    "        # If nothing changes then stop\n",
    "        if num_relevant == len(new_biginds): break\n",
    "        else: num_relevant = len(new_biginds)\n",
    "            \n",
    "        # Also make sure we didn't just lose all the coefficients\n",
    "        if len(new_biginds) == 0:\n",
    "            if j == 0: \n",
    "                #if print_results: print \"Tolerance too high - all coefficients set below tolerance\"\n",
    "                return w\n",
    "            else: break\n",
    "        biginds = new_biginds\n",
    "        \n",
    "        # Otherwise get a new guess\n",
    "        w[smallinds] = 0\n",
    "        if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y))[0]\n",
    "        else: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
    "\n",
    "    # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
    "    \n",
    "    if normalize != 0: return np.multiply(Mreg,w)\n",
    "    else: return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe2e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10 #m轮更新\n",
    "b = 1.2 #叛逃者的收益\n",
    "K = 0.1\n",
    "P_PDG = array([[1,0],[b,0]]) #2*2的收益矩阵\n",
    "Noise = 1\n",
    "W = np.loadtxt(\"dolphin.txt\") \n",
    "T = W.shape[0] #全部社区内个体的总数量\n",
    "SV = 6\n",
    "# the part for generate EG data\n",
    "y = zeros((SV*M,T))\n",
    "AA = zeros((T,SV*M,T))\n",
    "for i in range(SV): #产生6组A和Y的值\n",
    "    [A, Aa] = data_generate(M,P_PDG,W,T,K,Noise)\n",
    "    for j in range(M):\n",
    "        y[i*M+j,:] =Aa[j,:]\n",
    "        for k in range(T):\n",
    "            AA[k,i*M+j,:] = A[k,j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0321a657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.85700616e-01 3.29047210e-01 4.37030910e-01 8.89856226e-01\n",
      "  9.83431212e-01 7.75729090e-01 8.48384779e-01 3.25999329e-02\n",
      "  2.80335416e-01 3.12823270e-01 9.50142474e-01 3.78196877e-01\n",
      "  3.74948897e-01 7.46512464e-01 5.56426745e-01 5.59424254e-01\n",
      "  9.01211485e-01 5.56387563e-01 8.86094640e-01 4.67095832e-01\n",
      "  5.18195047e-01 4.72031708e-02 6.81496721e-01 2.50238321e-01\n",
      "  1.20174248e-01 4.33951122e-01 8.25972489e-02 2.48182621e-01\n",
      "  6.85106517e-02 2.01799124e-01 2.74200452e-01 6.79823033e-01\n",
      "  5.88082386e-01 7.35785310e-01 3.67471741e-01 5.33809118e-01\n",
      "  4.72918413e-01 8.18045797e-01 2.10189652e-01 6.00155569e-01\n",
      "  3.01286580e-01 5.90871031e-01 9.45321300e-01 6.77964333e-01\n",
      "  1.37709450e-01 8.69463780e-01 9.80566033e-01 7.32921698e-01\n",
      "  4.68336211e-01 4.06222311e-01 6.22266831e-01 8.84500578e-01\n",
      "  5.98027547e-01 4.56232478e-01 9.28899516e-01 7.15842715e-01\n",
      "  6.61471730e-01 9.98485663e-01 8.51504479e-01 7.75459015e-01\n",
      "  2.58348250e-01 1.96901484e-01]\n",
      " [8.71305592e-01 4.62831458e-01 4.13917718e-01 7.72656708e-01\n",
      "  6.67924957e-01 8.23008428e-01 4.58216091e-02 5.23033741e-01\n",
      "  4.52056574e-01 9.58813336e-01 2.24949408e-01 3.65840528e-01\n",
      "  8.04148821e-01 2.15178619e-01 5.25612421e-01 7.04881973e-01\n",
      "  7.12594808e-01 6.67000974e-01 4.16910040e-01 4.44353049e-01\n",
      "  5.84914035e-01 4.78245876e-01 1.39095567e-01 5.80748097e-01\n",
      "  7.47262298e-02 3.35254859e-01 9.48688780e-01 1.64515067e-01\n",
      "  1.35366632e-01 6.48008607e-01 6.37441370e-01 1.44115545e-01\n",
      "  1.01461147e-01 1.73165519e-02 8.88236978e-01 4.49313777e-01\n",
      "  7.25287095e-01 9.53726324e-01 5.05285281e-02 1.65718632e-01\n",
      "  7.40262556e-01 1.78936714e-01 1.29686181e-01 9.72070227e-01\n",
      "  3.47317389e-01 6.24836322e-01 4.45942786e-01 5.46144817e-01\n",
      "  7.34136672e-01 2.62350234e-01 2.41621682e-01 9.90126748e-02\n",
      "  2.95138660e-01 2.32688063e-01 3.74293381e-01 5.06417120e-01\n",
      "  6.64568284e-02 2.55863109e-02 2.69073964e-01 3.09722860e-01\n",
      "  7.09323123e-01 2.92452779e-02]\n",
      " [1.78830063e-01 2.99567100e-02 7.81218673e-01 1.83205176e-01\n",
      "  5.39727945e-01 5.34942077e-01 8.89864244e-02 7.71075258e-01\n",
      "  6.44862097e-01 3.95022006e-01 7.73696457e-01 3.37130965e-01\n",
      "  7.74501017e-01 7.94350842e-01 9.61482254e-02 1.44646521e-01\n",
      "  8.26670549e-01 3.65476374e-01 4.22946071e-01 9.24956067e-02\n",
      "  7.18074647e-01 6.40373616e-01 4.18775615e-01 6.82721252e-01\n",
      "  8.97521612e-01 9.17312666e-01 7.56345929e-02 4.67200991e-01\n",
      "  6.72181146e-01 4.53411072e-01 9.86467336e-01 4.37505243e-01\n",
      "  2.53475148e-01 4.49112932e-01 9.25313026e-01 6.70939262e-01\n",
      "  1.77313452e-01 8.80004139e-01 3.13416128e-01 7.20400095e-01\n",
      "  4.15507106e-02 1.70940205e-01 5.39974253e-01 7.46243549e-01\n",
      "  2.95883200e-01 9.25669756e-01 4.50556034e-01 5.07004589e-01\n",
      "  8.29622066e-01 4.44797597e-01 4.38977146e-01 7.78357499e-01\n",
      "  7.96418799e-01 4.13500339e-01 7.41737282e-02 1.81204779e-01\n",
      "  5.23352964e-01 5.64611867e-03 3.47308708e-01 2.79374066e-01\n",
      "  7.59076249e-01 5.16480430e-01]\n",
      " [9.87047702e-01 8.38596505e-01 2.62239215e-01 5.38815860e-01\n",
      "  5.45220905e-01 6.75356112e-01 5.12989680e-01 4.40606766e-01\n",
      "  2.38894158e-01 3.69120568e-01 2.09288616e-01 8.39484410e-01\n",
      "  4.30703334e-01 7.23567993e-01 2.64196371e-01 9.94663450e-01\n",
      "  7.45196487e-01 3.59713751e-01 8.26893973e-01 2.54924326e-01\n",
      "  2.69346285e-01 7.08227866e-01 2.14896850e-01 8.72098306e-01\n",
      "  6.22001216e-01 6.87368578e-01 3.17023097e-01 4.55858280e-01\n",
      "  9.38667188e-01 5.22942171e-01 9.76720026e-01 5.62408036e-01\n",
      "  6.79412674e-01 9.23662682e-02 2.96858165e-01 5.28084912e-01\n",
      "  7.67719301e-01 2.69258686e-01 3.94204318e-01 5.39726090e-01\n",
      "  5.53248977e-01 7.36886526e-01 9.07543834e-02 4.32745985e-02\n",
      "  1.60068366e-01 5.48527199e-01 6.80347846e-01 3.01979716e-01\n",
      "  5.09586208e-01 1.14301194e-01 3.63610462e-01 7.46621935e-01\n",
      "  5.65758220e-01 5.84004749e-01 8.29305983e-01 2.86749774e-01\n",
      "  2.60104805e-02 2.79692392e-01 1.10228001e-01 6.04638260e-01\n",
      "  6.40927191e-01 8.13185270e-01]\n",
      " [4.62256957e-01 4.11761499e-01 7.43036010e-01 4.71671217e-01\n",
      "  3.26418992e-01 9.31257712e-01 7.04945643e-01 2.99859913e-01\n",
      "  7.74458567e-01 4.38601236e-02 8.92001768e-01 6.43016185e-01\n",
      "  4.67933754e-01 9.61354972e-01 2.99952902e-01 9.89686428e-01\n",
      "  1.41007194e-01 3.00229396e-02 6.59288537e-01 9.22363850e-01\n",
      "  7.83080150e-01 4.75700878e-01 6.05178023e-01 8.92249289e-01\n",
      "  2.26372898e-01 4.12878297e-01 8.60232444e-01 8.96975765e-01\n",
      "  8.74431202e-01 2.27092576e-01 8.56265473e-02 5.20810632e-01\n",
      "  1.95169260e-01 1.99105110e-02 5.43364858e-01 9.80346788e-01\n",
      "  1.32626677e-01 9.41892978e-01 6.49948550e-01 2.16891457e-01\n",
      "  8.93237708e-01 3.99338461e-01 5.51691176e-01 2.47510566e-01\n",
      "  3.68179054e-01 3.42676067e-01 1.36219792e-01 1.99959760e-01\n",
      "  8.41696802e-01 7.91896967e-01 7.43560073e-01 5.95859166e-01\n",
      "  9.57483292e-01 3.96478304e-01 7.45150068e-01 7.07030559e-02\n",
      "  6.70821767e-01 8.43050733e-02 5.63437129e-01 1.20989715e-01\n",
      "  9.50297626e-01 2.66780903e-01]\n",
      " [6.01084007e-01 3.08325246e-01 3.53118413e-01 1.22746138e-01\n",
      "  8.36293398e-01 9.75015161e-01 5.31186650e-02 3.76950688e-01\n",
      "  5.39111201e-01 6.50633615e-02 1.43366673e-01 8.89754040e-01\n",
      "  3.67428179e-01 9.86733481e-01 5.17487382e-01 4.70145901e-01\n",
      "  2.75476561e-01 8.62766211e-02 8.25879753e-01 8.80905062e-02\n",
      "  5.01586594e-01 4.24006018e-01 6.76986199e-01 7.29086628e-01\n",
      "  2.27738372e-01 9.26181289e-01 5.34488264e-01 2.24175396e-01\n",
      "  1.36076029e-01 3.20277572e-01 7.96311057e-01 6.35757576e-02\n",
      "  9.43449162e-01 2.59469984e-01 4.98448503e-01 2.60201012e-02\n",
      "  7.95168732e-01 6.80170799e-01 6.15820813e-01 2.43405245e-01\n",
      "  3.55616097e-01 2.79762386e-01 7.99129547e-01 1.26722947e-01\n",
      "  4.43386720e-01 9.17801100e-02 3.94523526e-01 2.80482496e-01\n",
      "  3.72437892e-01 5.65295473e-02 7.65159557e-01 9.43930452e-01\n",
      "  6.58307541e-02 7.91005990e-01 5.72019042e-02 9.56405714e-01\n",
      "  4.39704100e-01 6.89009620e-01 8.01516986e-02 9.40346879e-01\n",
      "  5.99216743e-01 1.24595898e-01]\n",
      " [5.52954030e-01 4.51611407e-01 9.41529119e-01 3.98314258e-01\n",
      "  7.17414362e-01 3.66005461e-01 3.20202289e-01 6.72206333e-01\n",
      "  9.27820944e-01 9.88185306e-02 2.49515990e-01 7.60694387e-01\n",
      "  8.74868394e-01 5.10636924e-01 9.94159340e-01 6.70674305e-01\n",
      "  2.84730159e-01 8.36337378e-01 5.53123221e-02 1.01863785e-01\n",
      "  6.33946937e-01 9.32896168e-01 6.68148166e-02 7.35593692e-02\n",
      "  4.52444241e-02 5.38688588e-01 1.12602189e-01 7.02947599e-01\n",
      "  8.81169182e-01 3.74807417e-01 4.05755757e-02 1.17861409e-01\n",
      "  6.83233213e-01 1.02651418e-01 8.79012443e-01 2.50688659e-01\n",
      "  4.70827677e-04 1.35763819e-01 4.10192295e-01 8.57690989e-01\n",
      "  7.18117149e-01 4.48596934e-01 1.07532018e-01 9.31486912e-01\n",
      "  4.21985702e-01 5.24322839e-01 3.47062983e-01 8.23001650e-01\n",
      "  5.06427365e-01 1.46368326e-01 4.72064426e-01 5.63504288e-01\n",
      "  5.13301869e-01 1.83419216e-01 8.49346820e-01 8.92346186e-01\n",
      "  4.82582108e-01 7.45365123e-01 8.96750298e-01 4.73956165e-01\n",
      "  3.36749348e-01 3.18917073e-01]\n",
      " [7.85054492e-01 1.62231380e-01 5.74452840e-01 2.03891344e-01\n",
      "  4.89865348e-01 9.72437228e-01 5.31080005e-01 7.83019255e-01\n",
      "  9.37487835e-01 2.95469152e-01 8.03282738e-01 5.90025127e-01\n",
      "  3.82330357e-01 3.85531985e-01 2.51736004e-01 8.33674863e-01\n",
      "  6.78453440e-01 3.21585370e-01 8.25146407e-01 8.56055598e-01\n",
      "  1.83405311e-01 8.14303927e-01 1.74653093e-01 3.69064276e-01\n",
      "  7.40565899e-01 5.24616563e-01 2.26239747e-02 2.47622211e-02\n",
      "  9.38901522e-01 9.21951507e-01 5.41755518e-01 9.29580754e-01\n",
      "  5.47947545e-02 2.63159972e-01 5.41524792e-01 5.00409573e-01\n",
      "  7.19047362e-01 1.71539839e-01 5.34241993e-01 2.52608571e-01\n",
      "  3.42130141e-01 9.26357125e-01 3.12930351e-01 6.56036591e-01\n",
      "  2.60144821e-01 9.92597732e-02 6.77427773e-01 5.71958731e-01\n",
      "  5.48623910e-01 6.54595009e-01 7.84262162e-01 4.99848146e-01\n",
      "  3.65351800e-01 5.52884142e-01 2.73122015e-01 5.24142564e-01\n",
      "  8.26958381e-01 6.57399626e-01 4.71976307e-01 3.40293203e-01\n",
      "  3.53080317e-01 1.24269383e-01]\n",
      " [9.54307844e-01 5.96111262e-01 8.41133856e-01 2.31271258e-02\n",
      "  8.52520790e-01 1.79819956e-01 8.83013925e-01 3.43614832e-01\n",
      "  7.29107723e-01 1.05732059e-01 7.59752636e-02 5.47817350e-01\n",
      "  7.36683990e-01 3.97949951e-01 6.36413589e-02 7.86783358e-01\n",
      "  2.77651052e-01 9.14562367e-01 4.19263687e-01 2.22492579e-01\n",
      "  8.76423328e-01 3.25820859e-01 7.86616536e-01 9.02109886e-01\n",
      "  8.58347266e-01 9.26925199e-01 1.13919537e-01 8.81035623e-01\n",
      "  7.25742136e-01 4.25012914e-01 9.91362874e-01 5.94526489e-02\n",
      "  9.77534711e-02 2.59523836e-02 7.44110646e-01 9.24960158e-01\n",
      "  7.42223101e-01 5.19748707e-01 2.22342580e-01 8.37622258e-01\n",
      "  3.59288922e-01 7.51612947e-01 5.60768162e-01 7.66145834e-01\n",
      "  6.06013929e-01 7.41650513e-01 1.28850203e-01 1.68560224e-01\n",
      "  4.10649216e-01 4.01754676e-01 3.58593590e-01 8.09468187e-01\n",
      "  5.88322183e-02 6.10771552e-01 9.86013252e-01 7.23075198e-01\n",
      "  2.89887574e-01 3.45525802e-01 7.13838754e-01 4.46757264e-01\n",
      "  2.59682820e-01 5.23454801e-01]\n",
      " [6.14574171e-01 9.74237360e-01 9.72843702e-01 9.74930765e-01\n",
      "  5.91665201e-01 8.17073948e-01 1.74331122e-01 5.39026068e-01\n",
      "  2.02461305e-01 1.82960783e-01 7.42097756e-01 1.21642661e-01\n",
      "  1.23608090e-01 6.19606771e-01 1.59531457e-01 1.70898525e-01\n",
      "  3.51329037e-01 9.79515841e-01 8.90197575e-01 5.94118578e-01\n",
      "  3.44566947e-01 7.67560693e-01 1.13527338e-01 5.48086791e-01\n",
      "  8.08653211e-01 1.16987099e-01 4.96322332e-01 1.06138842e-01\n",
      "  1.34508078e-01 4.71939318e-01 2.99981762e-01 3.47350532e-01\n",
      "  2.80567560e-02 1.25120499e-01 9.37823823e-01 6.60074099e-01\n",
      "  3.12654032e-01 9.68312192e-01 9.88637012e-01 2.59055293e-01\n",
      "  8.11883470e-01 5.69573065e-01 3.19561415e-01 8.06026847e-01\n",
      "  3.11238532e-01 4.14327793e-01 2.34139353e-02 5.33843380e-01\n",
      "  2.67482740e-01 9.59523798e-02 6.56245634e-01 1.18205479e-01\n",
      "  4.69200012e-01 8.21730202e-01 1.34628218e-01 4.30628097e-01\n",
      "  3.72809787e-02 3.81729685e-01 4.50188177e-01 1.18598339e-01\n",
      "  9.66980656e-02 5.21426375e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(Noise*random.random((M,T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf2aa481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-13ac3e2d7f51>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-3-13ac3e2d7f51>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y))[0]\n",
      "<ipython-input-10-0f2aa95661a7>:5: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n",
      "<ipython-input-3-13ac3e2d7f51>:102: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y))[0]\n",
      "<ipython-input-3-13ac3e2d7f51>:106: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "for j in range(T):\n",
    "    w = TrainSTRidge(AA[j], y[:,j].reshape(-1,1),10**2,1)\n",
    "    for i in range(w.shape[0]):\n",
    "        X[i,j]=w[i,0]\n",
    "np.savetxt(\"X.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b210bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge_SPL(R, Ut, v, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "    \"\"\"\n",
    "    This function trains a predictor using STRidge_SPL.\n",
    "\n",
    "    It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "    using a loss function on a holdout set.\n",
    "\n",
    "    Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "    not squared 2-norm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "    np.random.seed(0) # for consistancy\n",
    "    n,_ = R.shape\n",
    "    train = np.random.choice(n, int(n*split), replace = False)\n",
    "    test = [i for i in np.arange(n) if i not in train]\n",
    "    TrainR = R[train,:]\n",
    "    TestR = R[test,:]\n",
    "    TrainY = Ut[train,:]\n",
    "    TestY = Ut[test,:]\n",
    "    D = TrainR.shape[1]       \n",
    "\n",
    "    # Set up the initial tolerance and l0 penalty\n",
    "    d_tol = float(d_tol)\n",
    "    tol = d_tol\n",
    "    if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
    "\n",
    "    # Get the standard least squares estimator\n",
    "    w = np.zeros((D,1))\n",
    "    w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
    "    err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "    tol_best = 0\n",
    "\n",
    "    # Now increase tolerance until test performance decreases\n",
    "    for iter in range(maxit):\n",
    "\n",
    "        # Get a set of coefficients and error\n",
    "        w = STRidge_SPL(R,Ut,v,lam,STR_iters,tol,normalize = normalize)\n",
    "        err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "\n",
    "        # Has the accuracy improved?\n",
    "        if err <= err_best:\n",
    "            err_best = err\n",
    "            w_best = w\n",
    "            tol_best = tol\n",
    "            tol = tol + d_tol\n",
    "\n",
    "        else:\n",
    "            tol = max([0,tol - 2*d_tol])\n",
    "            d_tol  = 2*d_tol / (maxit - iter)\n",
    "            tol = tol + d_tol\n",
    "\n",
    "    if print_best_tol: \n",
    "        print (\"Optimal tolerance:\", tol_best)\n",
    "    return w_best\n",
    "\n",
    "def STRidge_SPL(X0, y, v, lam, maxit, tol, normalize = 2, print_results = False):\n",
    "    \"\"\"\n",
    "    Sequential Threshold Ridge Regression algorithm for finding (hopefully) sparse \n",
    "    approximation to X^{-1}y.  The idea is that this may do better with correlated observables.\n",
    "\n",
    "    This assumes y is only one column\n",
    "    \"\"\"\n",
    "\n",
    "    n,d = X0.shape\n",
    "    X = np.zeros((n,d), dtype=np.complex64)\n",
    "    # First normalize data\n",
    "    if normalize != 0:\n",
    "        Mreg = np.zeros((d,1))\n",
    "        for i in range(0,d):\n",
    "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
    "            X[:,i] = Mreg[i]*X0[:,i]\n",
    "    else: X = X0\n",
    "    \n",
    "    # Get the standard ridge esitmate\n",
    "    if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
    "    else: w = np.linalg.lstsq(X*v,y*v)[0]\n",
    "    num_relevant = d\n",
    "    biginds = np.where( abs(w) > tol)[0]\n",
    "    \n",
    "    # Threshold and continue\n",
    "    for j in range(maxit):\n",
    "\n",
    "        # Figure out which items to cut out\n",
    "        smallinds = np.where( abs(w) < tol)[0]\n",
    "        new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "            \n",
    "        # If nothing changes then stop\n",
    "        if num_relevant == len(new_biginds): break\n",
    "        else: num_relevant = len(new_biginds)\n",
    "            \n",
    "        # Also make sure we didn't just lose all the coefficients\n",
    "        if len(new_biginds) == 0:\n",
    "            if j == 0: \n",
    "                #if print_results: print \"Tolerance too high - all coefficients set below tolerance\"\n",
    "                return w\n",
    "            else: break\n",
    "        biginds = new_biginds\n",
    "        \n",
    "        # Otherwise get a new guess\n",
    "        w[smallinds] = 0\n",
    "        if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n",
    "        else: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
    "\n",
    "    # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
    "    \n",
    "    if normalize != 0: return np.multiply(Mreg,w)\n",
    "    else: return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c4d6f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-5e04d6e95ebc>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-11-5e04d6e95ebc>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
      "<ipython-input-62-5cb14215ff6f>:18: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n",
      "<ipython-input-11-5e04d6e95ebc>:102: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n",
      "<ipython-input-11-5e04d6e95ebc>:106: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
      "<ipython-input-11-5e04d6e95ebc>:106: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "v = ones((SV*M,1))\n",
    "v1 = zeros((SV*M,1))\n",
    "Los = zeros((SV*M,T))\n",
    "losss = zeros((SV*M,1))\n",
    "lambda_0=0.6\n",
    "t=0\n",
    "while (t<26):  \n",
    "    if (v==v1).all():\n",
    "        break\n",
    "    else:\n",
    "        for j in range(T):\n",
    "            w = TrainSTRidge_SPL(AA[j], y[:,j].reshape(-1,1), v,10**2,1)\n",
    "            Lo =  abs((np.dot(AA[j], w))-y[:,j].reshape(-1,1))\n",
    "            for i in range(SV*M):\n",
    "                Los[i,j]=Lo[i]\n",
    "            for i in range(w.shape[0]):\n",
    "                X[i,j]=w[i,0]\n",
    "        for i in range(SV*M):\n",
    "            losss[i]=np.mean(Los[i,:])\n",
    "        t = t+1\n",
    "        for i in range(SV*M):\n",
    "            if losss[i]<lambda_0:\n",
    "                v[i]=1-losss[i]/lambda_0\n",
    "            else:\n",
    "                v[i]=0\n",
    "        lambda_0 = lambda_0*1.25\n",
    "        #lambda_0 = lambda_0 + np.mean(losss)      \n",
    "np.savetxt(\"X1.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff62e5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99253045],\n",
       "       [0.99294147],\n",
       "       [0.99359947],\n",
       "       [0.99179317],\n",
       "       [0.99354502],\n",
       "       [0.99313858],\n",
       "       [0.99371022],\n",
       "       [0.9928882 ],\n",
       "       [0.99370996],\n",
       "       [0.99338871],\n",
       "       [0.99541893],\n",
       "       [0.99494254],\n",
       "       [0.99535845],\n",
       "       [0.99667092],\n",
       "       [0.99659845],\n",
       "       [0.99716099],\n",
       "       [0.99746786],\n",
       "       [0.99712872],\n",
       "       [0.9975435 ],\n",
       "       [0.99736025],\n",
       "       [0.99485072],\n",
       "       [0.99515991],\n",
       "       [0.99528924],\n",
       "       [0.99553386],\n",
       "       [0.99646897],\n",
       "       [0.99564638],\n",
       "       [0.99642721],\n",
       "       [0.99510037],\n",
       "       [0.99548497],\n",
       "       [0.99474373],\n",
       "       [0.99263551],\n",
       "       [0.9948208 ],\n",
       "       [0.99468938],\n",
       "       [0.99659058],\n",
       "       [0.99727228],\n",
       "       [0.9966898 ],\n",
       "       [0.99718577],\n",
       "       [0.99664544],\n",
       "       [0.99671865],\n",
       "       [0.99678126],\n",
       "       [0.99504831],\n",
       "       [0.99446114],\n",
       "       [0.99478377],\n",
       "       [0.99368563],\n",
       "       [0.99459324],\n",
       "       [0.99505519],\n",
       "       [0.99537382],\n",
       "       [0.99544654],\n",
       "       [0.99686659],\n",
       "       [0.99693792],\n",
       "       [0.99483296],\n",
       "       [0.9958163 ],\n",
       "       [0.99453783],\n",
       "       [0.99474158],\n",
       "       [0.99461792],\n",
       "       [0.99392718],\n",
       "       [0.99434018],\n",
       "       [0.99450831],\n",
       "       [0.99354639],\n",
       "       [0.99316511]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e00da151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-5e04d6e95ebc>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-11-5e04d6e95ebc>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
      "<ipython-input-74-46954bfc1b17>:17: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n",
      "<ipython-input-11-5e04d6e95ebc>:102: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n",
      "<ipython-input-11-5e04d6e95ebc>:106: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
      "<ipython-input-11-5e04d6e95ebc>:106: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "v = ones((SV*M,T))\n",
    "v1 = zeros((SV*M,T))\n",
    "Los = zeros((SV*M,T))\n",
    "lambda_0=1\n",
    "t=0\n",
    "while (t<25):\n",
    "    if (v==v1).all():\n",
    "        break\n",
    "    else:\n",
    "        for j in range(T):\n",
    "            w = TrainSTRidge_SPL(AA[j], y[:,j].reshape(-1,1), v[:,j].reshape(-1,1),10**2,1)\n",
    "            Lo =  abs((np.dot(AA[j], w))-y[:,j].reshape(-1,1))\n",
    "            for i in range(SV*M):\n",
    "                Los[i,j]=Lo[i]\n",
    "            for i in range(w.shape[0]):\n",
    "                X[i,j]=w[i,0]\n",
    "        t = t+1\n",
    "        for j in range(T):\n",
    "            for i in range(SV*M):\n",
    "                if Los[i,j]<lambda_0:\n",
    "                    v[i,j]=1-Los[i,j]/lambda_0\n",
    "                else:\n",
    "                    v[i,j]=0\n",
    "        lambda_0 = lambda_0*1.25\n",
    "        #lambda_0 = lambda_0 + np.mean(Los)\n",
    "np.savetxt(\"X2.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "15d2e7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99360852, 0.99182599, 0.99306591, ..., 0.99611834, 0.9996592 ,\n",
       "        0.99598325],\n",
       "       [0.99269985, 0.98641488, 0.99110899, ..., 0.99333621, 0.99759457,\n",
       "        0.99468098],\n",
       "       [0.99941523, 0.98453523, 0.99335071, ..., 0.99822469, 0.99818597,\n",
       "        0.99998158],\n",
       "       ...,\n",
       "       [0.99440547, 0.9996112 , 0.99340701, ..., 0.99369005, 0.99806001,\n",
       "        0.99836005],\n",
       "       [0.98805081, 0.99906964, 0.99990961, ..., 0.9889458 , 0.9988205 ,\n",
       "        0.99472598],\n",
       "       [0.99679979, 0.99669256, 0.99440044, ..., 0.9926274 , 0.99999685,\n",
       "        0.99339232]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86360c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
