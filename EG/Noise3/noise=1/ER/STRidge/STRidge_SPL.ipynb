{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2990296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "from random import shuffle\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9b6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generate(M,P_PDG,W,N,K,Noise):\n",
    "    #W是原数据集,代表哪些代理之间有连接，即网络\n",
    "    #N是W的行数，即代理的个数\n",
    "    #代理人的博弈策略\n",
    "    player = zeros((M+1,N))\n",
    "    #原始博弈策略放在第一行，随机设置\n",
    "    for i in range(N):\n",
    "        if random.random()<=0.5:\n",
    "            player[0,i] = 1 #1代表合作\n",
    "        else:\n",
    "            player[0,i] = 0 #0代表不合作\n",
    "    #计算每个节点的收益\n",
    "    #M轮博弈\n",
    "    F = zeros((1,N))\n",
    "    G = zeros((M,N))\n",
    "    A = zeros((N,M,N))\n",
    "    for t in range(M):\n",
    "        for i in range(N):\n",
    "            if player[t,i] == 1:\n",
    "                s1 = array([[1],[0]])\n",
    "            else:\n",
    "                s1 = array([[0],[1]])\n",
    "            for j in range(N):\n",
    "                if player[t,j] == 0:\n",
    "                    s2 = array([[0],[1]])\n",
    "                else:\n",
    "                    s2 = array([[1],[0]])\n",
    "                F[0,j] = ((s1.T).dot(P_PDG)).dot(s2) #如果代理i和j连接，则代理i的收益为F[0,j]\n",
    "            A[i,t,:] = F  #A:N*M*N   \n",
    "            # F是三维矩阵A的第i页，第t行\n",
    "            G[t,i] = F.dot(W[:,i]) #第t轮代理i的收益\n",
    "        # update strategies\n",
    "        for k in range(N):\n",
    "            s=[i for i,x in enumerate(list(W[:,k])) if x>=1]     # 找出与代理k合作的代理的索引\n",
    "            if len(s)!=0: # 如果有代理与代理k合作\n",
    "                shuffle(s)\n",
    "                P = 1/(1+math.e**((G[t,k]-G[t,s[0]])/K)) # 费米规则\n",
    "                if random.random()<= P:\n",
    "                    player[t+1,k] = player[t,s[0]]\n",
    "                else:\n",
    "                    player[t+1,k] = player[t,k]\n",
    "            else: # 如果没有代理与代理k合作\n",
    "                player[t+1,k] = player[t,k]\n",
    "    # add noise for G\n",
    "    Aa = G + Noise*random.random((M,N)) # m轮收益总矩阵：包含每一轮的收益M*N\n",
    "    return [A,Aa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40594aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge(R, Ut, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "    \"\"\"\n",
    "    This function trains a predictor using STRidge.\n",
    "\n",
    "    It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "    using a loss function on a holdout set.\n",
    "\n",
    "    Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "    not squared 2-norm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "    np.random.seed(0) # for consistancy\n",
    "    n,_ = R.shape\n",
    "    train = np.random.choice(n, int(n*split), replace = False)\n",
    "    test = [i for i in np.arange(n) if i not in train]\n",
    "    TrainR = R[train,:]\n",
    "    TestR = R[test,:]\n",
    "    TrainY = Ut[train,:]\n",
    "    TestY = Ut[test,:]\n",
    "    D = TrainR.shape[1]       \n",
    "\n",
    "    # Set up the initial tolerance and l0 penalty\n",
    "    d_tol = float(d_tol)\n",
    "    tol = d_tol\n",
    "    if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
    "\n",
    "    # Get the standard least squares estimator\n",
    "    w = np.zeros((D,1))\n",
    "    w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
    "    err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "    tol_best = 0\n",
    "\n",
    "    # Now increase tolerance until test performance decreases\n",
    "    for iter in range(maxit):\n",
    "\n",
    "        # Get a set of coefficients and error\n",
    "        w = STRidge(R,Ut,lam,STR_iters,tol,normalize = normalize)\n",
    "        err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "\n",
    "        # Has the accuracy improved?\n",
    "        if err <= err_best:\n",
    "            err_best = err\n",
    "            w_best = w\n",
    "            tol_best = tol\n",
    "            tol = tol + d_tol\n",
    "\n",
    "        else:\n",
    "            tol = max([0,tol - 2*d_tol])\n",
    "            d_tol  = 2*d_tol / (maxit - iter)\n",
    "            tol = tol + d_tol\n",
    "\n",
    "    if print_best_tol: \n",
    "        print (\"Optimal tolerance:\", tol_best)\n",
    "    return w_best\n",
    "\n",
    "def STRidge(X0, y, lam, maxit, tol, normalize = 2, print_results = False):\n",
    "    \"\"\"\n",
    "    Sequential Threshold Ridge Regression algorithm for finding (hopefully) sparse \n",
    "    approximation to X^{-1}y.  The idea is that this may do better with correlated observables.\n",
    "\n",
    "    This assumes y is only one column\n",
    "    \"\"\"\n",
    "\n",
    "    n,d = X0.shape\n",
    "    X = np.zeros((n,d), dtype=np.complex64)\n",
    "    # First normalize data\n",
    "    if normalize != 0:\n",
    "        Mreg = np.zeros((d,1))\n",
    "        for i in range(0,d):\n",
    "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
    "            X[:,i] = Mreg[i]*X0[:,i]\n",
    "    else: X = X0\n",
    "    \n",
    "    # Get the standard ridge esitmate\n",
    "    if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y))[0]\n",
    "    else: w = np.linalg.lstsq(X,y)[0]\n",
    "    num_relevant = d\n",
    "    biginds = np.where( abs(w) > tol)[0]\n",
    "    \n",
    "    # Threshold and continue\n",
    "    for j in range(maxit):\n",
    "\n",
    "        # Figure out which items to cut out\n",
    "        smallinds = np.where( abs(w) < tol)[0]\n",
    "        new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "            \n",
    "        # If nothing changes then stop\n",
    "        if num_relevant == len(new_biginds): break\n",
    "        else: num_relevant = len(new_biginds)\n",
    "            \n",
    "        # Also make sure we didn't just lose all the coefficients\n",
    "        if len(new_biginds) == 0:\n",
    "            if j == 0: \n",
    "                #if print_results: print \"Tolerance too high - all coefficients set below tolerance\"\n",
    "                return w\n",
    "            else: break\n",
    "        biginds = new_biginds\n",
    "        \n",
    "        # Otherwise get a new guess\n",
    "        w[smallinds] = 0\n",
    "        if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y))[0]\n",
    "        else: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
    "\n",
    "    # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
    "    \n",
    "    if normalize != 0: return np.multiply(Mreg,w)\n",
    "    else: return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe2e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10 #m轮更新\n",
    "b = 1.2 #叛逃者的收益\n",
    "K = 0.1\n",
    "P_PDG = array([[1,0],[b,0]]) #2*2的收益矩阵\n",
    "Noise = 1\n",
    "W = np.loadtxt(\"ER.txt\") \n",
    "T = W.shape[0] #全部社区内个体的总数量\n",
    "SV = 6\n",
    "# the part for generate EG data\n",
    "y = zeros((SV*M,T))\n",
    "AA = zeros((T,SV*M,T))\n",
    "for i in range(SV): #产生6组A和Y的值\n",
    "    [A, Aa] = data_generate(M,P_PDG,W,T,K,Noise)\n",
    "    for j in range(M):\n",
    "        y[i*M+j,:] =Aa[j,:]\n",
    "        for k in range(T):\n",
    "            AA[k,i*M+j,:] = A[k,j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab403e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.77016949e-01 9.69646002e-02 5.62619981e-01 9.19299287e-01\n",
      "  4.11732776e-01 5.09005396e-01 6.59224737e-01 6.83503589e-01\n",
      "  4.52211981e-01 3.55556008e-01 9.35689180e-01 1.43615232e-01\n",
      "  9.79344428e-01 1.46370168e-01 9.99142528e-01 7.32240903e-01\n",
      "  3.39635612e-01 9.57661002e-01 5.03932165e-01 1.40972946e-01\n",
      "  5.72453796e-02 7.40625952e-01 7.44291719e-01 6.32922208e-01\n",
      "  4.92855782e-01 5.86078002e-01 4.52249292e-01 6.89023814e-01\n",
      "  8.18215874e-01 9.63160085e-01 3.01594778e-01 9.08158374e-01\n",
      "  2.20756645e-02 3.61602287e-01 3.41595278e-01 1.83661469e-01\n",
      "  2.88452435e-01 5.29479398e-02 1.21808823e-01 5.59197198e-01]\n",
      " [2.17860770e-01 7.89889105e-01 2.45244756e-01 1.07056273e-01\n",
      "  4.00725077e-01 5.17276135e-01 1.97371985e-01 9.03516954e-01\n",
      "  2.69154569e-01 4.77913370e-01 8.53543804e-01 3.35223989e-01\n",
      "  5.76065018e-01 2.02209576e-01 2.40316584e-01 3.85644353e-01\n",
      "  8.92635446e-02 4.98435910e-02 7.60627173e-01 7.69950897e-01\n",
      "  8.92741043e-01 5.99981645e-01 2.37521757e-01 4.05483311e-01\n",
      "  7.06106127e-01 3.17221859e-01 4.65106156e-01 2.62946092e-01\n",
      "  8.21983972e-01 2.88912293e-01 2.87408750e-01 2.56771193e-01\n",
      "  9.02204842e-01 7.10131317e-01 1.61229037e-01 3.30429228e-01\n",
      "  9.88137031e-01 6.35065135e-01 4.44534248e-02 4.69479825e-01]\n",
      " [6.94569813e-02 9.39269706e-01 5.47292769e-01 5.96728700e-01\n",
      "  6.81616801e-01 1.29087679e-01 7.44851300e-01 9.03878465e-02\n",
      "  2.75508215e-01 5.78046263e-01 8.13147271e-02 1.00792746e-01\n",
      "  2.95790879e-01 8.06685296e-01 5.05243848e-01 1.20346587e-01\n",
      "  9.93196774e-02 8.36669269e-01 9.66603476e-01 6.04264531e-02\n",
      "  5.00256489e-01 6.17515748e-01 2.63638015e-01 3.20104018e-01\n",
      "  3.20739944e-01 6.27201992e-01 7.10282022e-01 3.64219213e-01\n",
      "  4.26283428e-01 5.76224068e-01 9.93311934e-02 9.21674969e-01\n",
      "  6.25335497e-01 4.30539600e-01 5.61085619e-03 1.56077513e-01\n",
      "  4.93121132e-01 3.18830522e-01 7.33722110e-01 6.62126476e-01]\n",
      " [9.91505989e-03 3.29763198e-01 9.02383634e-01 6.08564799e-01\n",
      "  1.01439740e-01 1.00159697e-01 8.34718671e-01 8.96588295e-01\n",
      "  5.73692636e-01 2.49752803e-01 1.50000162e-01 1.17444989e-01\n",
      "  6.73685623e-01 1.14461453e-01 2.62235241e-01 3.54499120e-01\n",
      "  3.23794602e-01 1.33421066e-01 9.43995708e-01 3.39212037e-01\n",
      "  6.08216229e-01 8.33654524e-01 2.61370965e-01 2.69043171e-01\n",
      "  4.02650622e-01 4.74900196e-01 4.32007520e-01 7.72814463e-01\n",
      "  4.28018878e-01 9.11417065e-01 8.94387281e-02 3.85405533e-01\n",
      "  8.72676039e-01 2.61698082e-01 1.12574377e-01 5.77409028e-01\n",
      "  1.96071242e-01 8.73606923e-01 3.77183205e-01 6.17884049e-01]\n",
      " [6.06866282e-01 1.58783271e-02 5.37632425e-01 3.48521884e-01\n",
      "  5.72929312e-01 2.76898860e-02 7.14546500e-01 6.01348333e-01\n",
      "  4.80972599e-01 1.52849053e-01 9.09433378e-01 1.49993470e-01\n",
      "  5.40575366e-01 7.48276353e-01 8.62830283e-01 5.22654228e-02\n",
      "  6.16683695e-01 1.80426651e-01 2.76645132e-01 6.11626501e-01\n",
      "  7.36935844e-01 1.48688221e-02 6.14013954e-01 4.95863465e-01\n",
      "  6.52922060e-01 8.52907504e-02 8.33474410e-02 8.51257296e-01\n",
      "  1.42882773e-01 3.61260928e-01 4.97445338e-01 7.44427224e-01\n",
      "  4.43143892e-01 6.40367373e-01 4.13615408e-01 8.90676815e-01\n",
      "  9.36413900e-01 7.08145786e-02 1.78216261e-01 6.85857677e-01]\n",
      " [7.24669417e-01 2.74332723e-01 5.11504071e-01 2.98032094e-01\n",
      "  4.27897355e-01 9.11833064e-01 2.61235819e-01 1.46176772e-01\n",
      "  8.81723391e-01 9.98629002e-01 9.47114712e-01 2.08544842e-01\n",
      "  9.05203689e-01 9.99551798e-01 7.86875434e-02 3.34796179e-01\n",
      "  1.04555440e-01 8.63057193e-01 8.68865787e-01 5.88748633e-01\n",
      "  7.03542897e-01 5.75677314e-02 8.91040894e-01 4.65731079e-01\n",
      "  6.99101531e-01 1.92590662e-01 8.50883126e-01 2.62432162e-01\n",
      "  7.76970398e-01 3.12327852e-01 7.84394636e-01 2.01578622e-01\n",
      "  6.44913409e-01 9.66090679e-01 8.31442917e-02 8.42602729e-01\n",
      "  5.39058660e-01 7.71049710e-01 8.48948607e-01 7.91748235e-01]\n",
      " [7.65723642e-01 8.73676973e-01 3.68468711e-01 1.23452400e-01\n",
      "  8.58359478e-01 4.55589503e-01 7.22784723e-01 4.30892914e-01\n",
      "  3.21788100e-01 6.95351292e-01 4.14550757e-01 7.17534520e-02\n",
      "  6.97797320e-01 3.14852037e-01 1.70143769e-01 6.99288610e-01\n",
      "  3.78218878e-01 3.61248142e-01 6.31312490e-01 8.34317690e-01\n",
      "  4.65536954e-01 7.32365617e-01 4.74258411e-01 9.79127920e-01\n",
      "  5.05929147e-01 8.47666381e-01 1.12780197e-01 9.55000772e-01\n",
      "  9.36836600e-01 5.43584268e-01 1.53796704e-01 3.87882298e-01\n",
      "  8.85866912e-01 7.21043060e-01 3.84289284e-01 4.16572266e-01\n",
      "  1.13115773e-02 7.27530998e-01 8.03068869e-01 4.71580076e-01]\n",
      " [6.70002135e-01 4.32347775e-01 3.03220010e-01 5.99815593e-01\n",
      "  8.44512435e-01 5.43682632e-02 4.15000996e-01 2.03300790e-02\n",
      "  5.12558314e-01 5.76482537e-01 9.02654959e-01 6.38877144e-01\n",
      "  9.81604875e-01 8.17175482e-01 5.66806894e-01 3.79780723e-01\n",
      "  9.30641124e-01 6.86203500e-01 7.02940342e-01 9.49826831e-02\n",
      "  8.63227978e-02 1.44092209e-02 2.90084877e-01 4.69714961e-01\n",
      "  6.91448270e-01 1.75393598e-01 6.55840455e-01 7.37085943e-02\n",
      "  2.80732846e-01 3.67837133e-01 6.57524280e-01 1.54638403e-01\n",
      "  9.49945226e-01 9.64045712e-01 3.80542423e-01 8.41996612e-01\n",
      "  9.57428971e-02 4.27082406e-02 5.86470513e-01 7.45428734e-01]\n",
      " [8.84588507e-01 2.94206616e-01 6.96930777e-01 6.71599069e-02\n",
      "  4.42805457e-01 8.34871316e-01 8.15745703e-01 2.44713973e-01\n",
      "  5.06073651e-01 8.78413352e-01 6.94843833e-01 3.15340589e-01\n",
      "  3.47890568e-01 7.57579061e-01 6.60245625e-01 1.18180619e-01\n",
      "  8.10949204e-01 8.78481708e-01 6.09456434e-01 7.11038704e-01\n",
      "  3.21791386e-01 7.30402207e-01 3.03703185e-01 8.53107725e-01\n",
      "  6.82388178e-01 6.37352038e-01 5.76756189e-02 1.29003594e-01\n",
      "  4.72282223e-01 3.48465547e-01 5.98457702e-01 9.79692395e-02\n",
      "  3.72010751e-02 7.74515792e-01 7.13491390e-01 7.68655342e-02\n",
      "  1.04828270e-01 6.60563635e-01 8.72233909e-01 2.78164908e-02]\n",
      " [2.80055800e-01 1.64870599e-01 2.27718337e-01 6.60669927e-01\n",
      "  8.16377830e-01 9.42208916e-01 1.76931993e-01 8.84581429e-01\n",
      "  2.31744125e-01 8.75914725e-01 1.93651583e-02 3.55276084e-01\n",
      "  9.62335156e-04 3.78553222e-01 6.55492271e-01 8.52459001e-01\n",
      "  5.02843876e-01 7.30215493e-01 5.18736627e-01 7.31625682e-02\n",
      "  5.05426884e-01 7.96910679e-02 2.58026829e-01 6.13506100e-01\n",
      "  7.71104369e-01 4.35482492e-01 2.70416531e-01 7.42211199e-01\n",
      "  6.79494142e-01 6.28807462e-01 1.40639947e-01 2.60618507e-01\n",
      "  8.67050600e-01 4.42316493e-01 2.84034831e-01 2.61197609e-01\n",
      "  8.01627393e-01 7.82296761e-01 2.09649095e-01 7.58126856e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(Noise*random.random((M,T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf2aa481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-13ac3e2d7f51>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-3-13ac3e2d7f51>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y))[0]\n",
      "<ipython-input-3-13ac3e2d7f51>:102: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y))[0]\n",
      "<ipython-input-3-13ac3e2d7f51>:106: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
      "<ipython-input-12-0ca45682905e>:5: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n",
      "<ipython-input-3-13ac3e2d7f51>:106: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "for j in range(T):\n",
    "    w = TrainSTRidge(AA[j], y[:,j].reshape(-1,1),10**-1,1)\n",
    "    for i in range(w.shape[0]):\n",
    "        X[i,j]=w[i,0]\n",
    "np.savetxt(\"X.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b210bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge_SPL(R, Ut, v, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "    \"\"\"\n",
    "    This function trains a predictor using STRidge_SPL.\n",
    "\n",
    "    It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "    using a loss function on a holdout set.\n",
    "\n",
    "    Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "    not squared 2-norm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "    np.random.seed(0) # for consistancy\n",
    "    n,_ = R.shape\n",
    "    train = np.random.choice(n, int(n*split), replace = False)\n",
    "    test = [i for i in np.arange(n) if i not in train]\n",
    "    TrainR = R[train,:]\n",
    "    TestR = R[test,:]\n",
    "    TrainY = Ut[train,:]\n",
    "    TestY = Ut[test,:]\n",
    "    D = TrainR.shape[1]       \n",
    "\n",
    "    # Set up the initial tolerance and l0 penalty\n",
    "    d_tol = float(d_tol)\n",
    "    tol = d_tol\n",
    "    if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
    "\n",
    "    # Get the standard least squares estimator\n",
    "    w = np.zeros((D,1))\n",
    "    w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
    "    err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "    tol_best = 0\n",
    "\n",
    "    # Now increase tolerance until test performance decreases\n",
    "    for iter in range(maxit):\n",
    "\n",
    "        # Get a set of coefficients and error\n",
    "        w = STRidge_SPL(R,Ut,v,lam,STR_iters,tol,normalize = normalize)\n",
    "        err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "\n",
    "        # Has the accuracy improved?\n",
    "        if err <= err_best:\n",
    "            err_best = err\n",
    "            w_best = w\n",
    "            tol_best = tol\n",
    "            tol = tol + d_tol\n",
    "\n",
    "        else:\n",
    "            tol = max([0,tol - 2*d_tol])\n",
    "            d_tol  = 2*d_tol / (maxit - iter)\n",
    "            tol = tol + d_tol\n",
    "\n",
    "    if print_best_tol: \n",
    "        print (\"Optimal tolerance:\", tol_best)\n",
    "    return w_best\n",
    "\n",
    "def STRidge_SPL(X0, y, v, lam, maxit, tol, normalize = 2, print_results = False):\n",
    "    \"\"\"\n",
    "    Sequential Threshold Ridge Regression algorithm for finding (hopefully) sparse \n",
    "    approximation to X^{-1}y.  The idea is that this may do better with correlated observables.\n",
    "\n",
    "    This assumes y is only one column\n",
    "    \"\"\"\n",
    "\n",
    "    n,d = X0.shape\n",
    "    X = np.zeros((n,d), dtype=np.complex64)\n",
    "    # First normalize data\n",
    "    if normalize != 0:\n",
    "        Mreg = np.zeros((d,1))\n",
    "        for i in range(0,d):\n",
    "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
    "            X[:,i] = Mreg[i]*X0[:,i]\n",
    "    else: X = X0\n",
    "    \n",
    "    # Get the standard ridge esitmate\n",
    "    if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
    "    else: w = np.linalg.lstsq(X*v,y*v)[0]\n",
    "    num_relevant = d\n",
    "    biginds = np.where( abs(w) > tol)[0]\n",
    "    \n",
    "    # Threshold and continue\n",
    "    for j in range(maxit):\n",
    "\n",
    "        # Figure out which items to cut out\n",
    "        smallinds = np.where( abs(w) < tol)[0]\n",
    "        new_biginds = [i for i in range(d) if i not in smallinds]\n",
    "            \n",
    "        # If nothing changes then stop\n",
    "        if num_relevant == len(new_biginds): break\n",
    "        else: num_relevant = len(new_biginds)\n",
    "            \n",
    "        # Also make sure we didn't just lose all the coefficients\n",
    "        if len(new_biginds) == 0:\n",
    "            if j == 0: \n",
    "                #if print_results: print \"Tolerance too high - all coefficients set below tolerance\"\n",
    "                return w\n",
    "            else: break\n",
    "        biginds = new_biginds\n",
    "        \n",
    "        # Otherwise get a new guess\n",
    "        w[smallinds] = 0\n",
    "        if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n",
    "        else: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
    "\n",
    "    # Now that we have the sparsity pattern, use standard least squares to get w\n",
    "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
    "    \n",
    "    if normalize != 0: return np.multiply(Mreg,w)\n",
    "    else: return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4d6f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-5e04d6e95ebc>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-13-5e04d6e95ebc>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
      "<ipython-input-13-5e04d6e95ebc>:102: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n",
      "<ipython-input-13-5e04d6e95ebc>:106: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
      "<ipython-input-31-41738fd7b637>:18: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n",
      "<ipython-input-13-5e04d6e95ebc>:106: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "v = ones((SV*M,1))\n",
    "v1 = zeros((SV*M,1))\n",
    "Los = zeros((SV*M,T))\n",
    "losss = zeros((SV*M,1))\n",
    "lambda_0=0.4\n",
    "t=0\n",
    "while (t<17):  \n",
    "    if (v==v1).all():\n",
    "        break\n",
    "    else:\n",
    "        for j in range(T):\n",
    "            w = TrainSTRidge_SPL(AA[j], y[:,j].reshape(-1,1), v,10**-1,1)\n",
    "            Lo =  abs((np.dot(AA[j], w))-y[:,j].reshape(-1,1))\n",
    "            for i in range(SV*M):\n",
    "                Los[i,j]=Lo[i]\n",
    "            for i in range(w.shape[0]):\n",
    "                X[i,j]=w[i,0]\n",
    "        for i in range(SV*M):\n",
    "            losss[i]=np.mean(Los[i,:])\n",
    "        t = t+1\n",
    "        for i in range(SV*M):\n",
    "            if losss[i]<lambda_0:\n",
    "                v[i]=1-losss[i]/lambda_0\n",
    "            else:\n",
    "                v[i]=0\n",
    "        lambda_0 = lambda_0*1.25\n",
    "        #lambda_0 = lambda_0 + np.mean(losss)      \n",
    "np.savetxt(\"X1.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff62e5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9807606 ],\n",
       "       [0.97581318],\n",
       "       [0.97135989],\n",
       "       [0.96924906],\n",
       "       [0.97467371],\n",
       "       [0.97713187],\n",
       "       [0.97511135],\n",
       "       [0.97005203],\n",
       "       [0.96928941],\n",
       "       [0.97476396],\n",
       "       [0.97929046],\n",
       "       [0.98417832],\n",
       "       [0.98437366],\n",
       "       [0.9795002 ],\n",
       "       [0.98293466],\n",
       "       [0.9799375 ],\n",
       "       [0.98213872],\n",
       "       [0.97989889],\n",
       "       [0.98065116],\n",
       "       [0.98589888],\n",
       "       [0.98012246],\n",
       "       [0.98477033],\n",
       "       [0.98054091],\n",
       "       [0.98254147],\n",
       "       [0.97729873],\n",
       "       [0.97151906],\n",
       "       [0.97321441],\n",
       "       [0.97177624],\n",
       "       [0.97354018],\n",
       "       [0.97192969],\n",
       "       [0.97642335],\n",
       "       [0.98213355],\n",
       "       [0.97802651],\n",
       "       [0.98054252],\n",
       "       [0.97730124],\n",
       "       [0.97818592],\n",
       "       [0.98048269],\n",
       "       [0.98178768],\n",
       "       [0.97720555],\n",
       "       [0.98149926],\n",
       "       [0.98389451],\n",
       "       [0.98034817],\n",
       "       [0.98282753],\n",
       "       [0.97604517],\n",
       "       [0.97983948],\n",
       "       [0.98320249],\n",
       "       [0.97899431],\n",
       "       [0.97231243],\n",
       "       [0.97365668],\n",
       "       [0.97286843],\n",
       "       [0.98283156],\n",
       "       [0.98437923],\n",
       "       [0.98557453],\n",
       "       [0.98503185],\n",
       "       [0.98366338],\n",
       "       [0.98756702],\n",
       "       [0.98109304],\n",
       "       [0.97678217],\n",
       "       [0.9775671 ],\n",
       "       [0.979666  ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e00da151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-5e04d6e95ebc>:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
      "<ipython-input-13-5e04d6e95ebc>:76: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w = np.linalg.lstsq(X.T.dot(X*v) + lam*np.eye(d),X.T.dot(y*v))[0]\n",
      "<ipython-input-13-5e04d6e95ebc>:102: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]*v) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y*v))[0]\n",
      "<ipython-input-13-5e04d6e95ebc>:106: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n",
      "<ipython-input-66-107301413b5f>:17: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X[i,j]=w[i,0]\n",
      "<ipython-input-13-5e04d6e95ebc>:106: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds]*v,y*v)[0]\n"
     ]
    }
   ],
   "source": [
    "X=zeros((T,T))\n",
    "v = ones((SV*M,T))\n",
    "v1 = zeros((SV*M,T))\n",
    "Los = zeros((SV*M,T))\n",
    "lambda_0=0.3\n",
    "t=0\n",
    "while (t<20):\n",
    "    if (v==v1).all():\n",
    "        break\n",
    "    else:\n",
    "        for j in range(T):\n",
    "            w = TrainSTRidge_SPL(AA[j], y[:,j].reshape(-1,1), v[:,j].reshape(-1,1),10**-1,1)\n",
    "            Lo =  abs((np.dot(AA[j], w))-y[:,j].reshape(-1,1))\n",
    "            for i in range(SV*M):\n",
    "                Los[i,j]=Lo[i]\n",
    "            for i in range(w.shape[0]):\n",
    "                X[i,j]=w[i,0]\n",
    "        t = t+1\n",
    "        for j in range(T):\n",
    "            for i in range(SV*M):\n",
    "                if Los[i,j]<lambda_0:\n",
    "                    v[i,j]=1#-Los[i,j]/lambda_0\n",
    "                else:\n",
    "                    v[i,j]=0\n",
    "        lambda_0 = lambda_0*1.25\n",
    "        #lambda_0 = lambda_0 + np.mean(Los)\n",
    "np.savetxt(\"X2.txt\", X,fmt='%f',delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "15d2e7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86360c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
